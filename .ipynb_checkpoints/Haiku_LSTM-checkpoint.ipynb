{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku generator with LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haiku dataset https://www.kaggle.com/hjhalani30/haiku-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preparation and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd \n",
    "import sys\n",
    "import keras\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 124295 rows and 6 columns\n"
     ]
    }
   ],
   "source": [
    "dirname = os.getcwd()\n",
    "filename = \"all_haiku.csv\"\n",
    "path_to_dataset = os.path.join(dirname, filename)\n",
    "n_examples = 124295\n",
    "data = pd.read_csv(path_to_dataset, delimiter=',', nrows = n_examples)\n",
    "nRow, nCol = data.shape\n",
    "print(f'There are {nRow} rows and {nCol} columns')\n",
    "\n",
    "#cols 1-3 are 1st, 2nd, and 3rd lines of the haiku respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols 1-3 are 1st, 2nd, and 3rd lines of the haiku respectively\n",
    "\n",
    "X_1_0 = data[:,1]\n",
    "X_2_0 = data[:,2]\n",
    "X_3_0 = data[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = []\n",
    "X_2 = []\n",
    "X_3 = []\n",
    "for i in range(n_examples):\n",
    "    if type(X_1_0[i]) == str and type(X_2_0[i]) == str and type(X_3_0[i]) == str:\n",
    "        X_1.append(X_1_0[i])\n",
    "        X_2.append(X_2_0[i])\n",
    "        X_3.append(X_3_0[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data to get rid off \"-\" etc\n",
    "#to make it sensitive to lines, maybe we could implement a line break as some symbol e.g. \"|\" and merge the X's together\n",
    "punctuation = ['-', '--', '~', '—', '.', ';', ',', ':', '?', '!', '#', '(', ')', '<', '>', '*', '%', '{', '_', ']', '[', '@', '`', '\\xa0', '=', '\"', '&', '–', '…', '“', '”']\n",
    "\n",
    "end_of_line = '.'\n",
    "end_of_haiku = '/'\n",
    "\n",
    "for i in range(len(X_1)):\n",
    "    for p in punctuation:\n",
    "        if p in X_1[i]:\n",
    "            X_1[i] = X_1[i].replace(p, '')\n",
    "        if p in X_2[i]:\n",
    "            X_2[i] = X_2[i].replace(p, '')\n",
    "        if p in X_3[i]:\n",
    "            X_3[i] = X_3[i].replace(p, '')\n",
    "   \n",
    "    #delete redundant spaces\n",
    "    if X_1[i][-3:-1] == '   ':\n",
    "        X_1[i] = X_1[i].replace('   ', '')\n",
    "    elif X_1[i][-2:-1] == '  ':\n",
    "        X_1[i] = X_1[i].replace('  ', '')   \n",
    "    elif X_1[i][-1] == ' ':\n",
    "        X_1[i] = X_1[i].replace(' ', '')\n",
    "    \n",
    "    if X_2[i][-3:-1] == '   ':\n",
    "        X_2[i] = X_2[i].replace('   ', '')\n",
    "    elif X_2[i][-2:-1] == '  ':\n",
    "        X_2[i] = X_2[i].replace('  ', '')   \n",
    "    elif X_1[i][-1] == ' ':\n",
    "        X_2[i] = X_2[i].replace(' ', '')\n",
    "        \n",
    "    if X_3[i][-3:-1] == '   ':\n",
    "        X_3[i] = X_1[i].replace('   ', '')\n",
    "    elif X_3[i][-2:-1] == '  ':\n",
    "        X_3[i] = X_1[i].replace('  ', '')   \n",
    "    elif X_3[i][-1] == ' ':\n",
    "        X_1[i] = X_1[i].replace(' ', '')\n",
    "    \n",
    "    #we want lowercase letters only\n",
    "    #X_1[i] = X_1[i].lower()\n",
    "    #X_2[i] = X_2[i].lower()\n",
    "    #X_3[i] = X_3[i].lower()\n",
    "    \n",
    "    X_1[i] += end_of_line\n",
    "    X_2[i] += end_of_line\n",
    "    X_3[i] += end_of_line\n",
    "    X_3[i] += end_of_haiku\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the three lines together\n",
    "haikus = []\n",
    "for i in range(len(X_1)):\n",
    "    haikus.append(X_1[i]+X_2[i]+X_3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20343\n"
     ]
    }
   ],
   "source": [
    "short_haikus = []\n",
    "for i in range(len(haikus)):\n",
    "    if len(haikus[i]) < 60:\n",
    "        short_haikus.append(haikus[i])\n",
    "n_haikus = len(short_haikus)\n",
    "print(n_haikus)\n",
    "\n",
    "haikus = short_haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in range(n_haikus):\n",
    "    max_len = max(max_len, len(haikus[i]))\n",
    "    \n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create characters dict \n",
    "token_index = {}\n",
    "idx = -1\n",
    "for haiku in haikus:\n",
    "    for i in range(len(haiku)):\n",
    "        if haiku[i] not in token_index:\n",
    "            token_index[haiku[i]] = idx+1\n",
    "            idx += 1\n",
    "\n",
    "#add padding if you want all of the same length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(token_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have turned characters into tokens with corresponding indexes we can vectorise our training examples by one-hot-encoding them and subsequently creating 3D numpy arrays for X and Y:\n",
    "\n",
    "`X`: This is an (n_haikus, max_len, vocab_size) dimensional array.\n",
    "- We have max_len timesteps for each training example\n",
    "- At each time step the input is one of vocab_size possible values and is represented as a one-hot vector. e.g. X[i,t,:] is a one-hot vector representing the value of the i-th example at time t. \n",
    "\n",
    "`Y`: a (max_len, n_haikus, vocab_size) dimensional array\n",
    "\n",
    "We're using the previous values to predict the next value. Therefore each traing example Y is just training example X moved one step ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max_len - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((n_haikus, max_len, vocab_size), dtype = 'bool')\n",
    "Y = np.zeros((max_len, n_haikus, vocab_size), dtype = 'bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorise haikus - turn them into a list of char indexes\n",
    "haiku_vectors = []\n",
    "for i, haiku in enumerate(haikus):\n",
    "    haiku_vec = []\n",
    "    for j in range(len(haiku)):\n",
    "        haiku_vec.append(token_index.get(haiku[j]))\n",
    "    haiku_vectors.append(haiku_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = []\n",
    "Y_vec = []\n",
    "for i, vec in enumerate(haiku_vectors):\n",
    "    X_vec.append(vec[:-1])\n",
    "    Y_vec.append(vec[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vec in enumerate(X_vec):\n",
    "    for j, idx in enumerate(vec):\n",
    "        X[i, j, idx] = True\n",
    "        \n",
    "for i, vec in enumerate(Y_vec):\n",
    "    for j, idx in enumerate(vec):\n",
    "        Y[j, i, idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(23231, 58, 74)\n",
      "(58, 23231, 74)\n"
     ]
    }
   ],
   "source": [
    "print(X[0, :, :])\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building an LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of dimensions for hidden state (a) of each LSTM cell\n",
    "n_a = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement LSTM forward pass defined by the following:\n",
    "\n",
    "Forget gate:\n",
    "\n",
    "$$\\mathbf{\\Gamma}_f^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_f[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_f)\\tag{1} $$\n",
    "\n",
    "Candidate value:\n",
    "\n",
    "$$\\mathbf{\\tilde{c}}^{\\langle t \\rangle} = \\tanh\\left( \\mathbf{W}_{c} [\\mathbf{a}^{\\langle t - 1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{c} \\right) \\tag{3}$$\n",
    "\n",
    "Update gate:\n",
    "\n",
    "$$\\mathbf{\\Gamma}_i^{\\langle t \\rangle} = \\sigma(\\mathbf{W}_i[a^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_i)\\tag{2} $$ \n",
    "\n",
    "Cell state:\n",
    "$$ \\mathbf{c}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_f^{\\langle t \\rangle}* \\mathbf{c}^{\\langle t-1 \\rangle} + \\mathbf{\\Gamma}_{i}^{\\langle t \\rangle} *\\mathbf{\\tilde{c}}^{\\langle t \\rangle} \\tag{4} $$\n",
    "\n",
    "Output gate:\n",
    "$$ \\mathbf{\\Gamma}_o^{\\langle t \\rangle}=  \\sigma(\\mathbf{W}_o[\\mathbf{a}^{\\langle t-1 \\rangle}, \\mathbf{x}^{\\langle t \\rangle}] + \\mathbf{b}_{o})\\tag{5}$$ \n",
    "\n",
    "Hidden state:\n",
    "$$ \\mathbf{a}^{\\langle t \\rangle} = \\mathbf{\\Gamma}_o^{\\langle t \\rangle} * \\tanh(\\mathbf{c}^{\\langle t \\rangle})\\tag{6} $$\n",
    "\n",
    "Prediction:\n",
    "$$\\mathbf{y}^{\\langle t \\rangle}_{pred} = \\textrm{softmax}(\\mathbf{W}_{y} \\mathbf{a}^{\\langle t \\rangle} + \\mathbf{b}_{y})$$\n",
    "\n",
    "We want to generate new characters at test time predicting the next character based on the last character and the parameters (which can pass information about previous characters too) so we need to build LSTM architecture which implements such inference. So we use keras functional API to build a custom model.\n",
    "\n",
    "We build a model by stacking keras layers on top of one another:\n",
    "1. Input layer (specifying shape of input) \n",
    "- tensorflow.keras.layers.Input() class\n",
    "2. \n",
    "\n",
    "\n",
    "LSTM_cell:\n",
    "\n",
    "Takes hidden state as input and returns the next hidden state\n",
    "\n",
    "dense_layer:\n",
    "\n",
    "Dense implements the operation: output = activation(dot(input, kernel) + bias) where:\n",
    "- activation = element-wise activation function passed as the activation argument\n",
    "- kernel = weights matrix created by the layer\n",
    "- bias = bias vector created by the layer \n",
    "\n",
    "Thus here we use Dense to calculated y_pred = softmax(dot(Wya, a) + by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the layer objects of the model globally so that they are fixed when used in a model function\n",
    "\n",
    "LSTM_cell = LSTM(n_a, return_state = True)\n",
    "\n",
    "prediction_layer = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "reshape_layer = Reshape((1, vocab_size)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(max_len, n_a, vocab_size):\n",
    "    \n",
    "    #specify the shape of input X\n",
    "    X = Input(shape=(max_len, vocab_size))\n",
    "    \n",
    "    #specify the size of accepted a0 and c) inputs\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    \n",
    "    #initiliase cell state and hidden state\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    #create an empty list to append predictions (outputs of Dense)\n",
    "    output = []\n",
    "    \n",
    "    for t in range(max_len):\n",
    "        \n",
    "        #adds a layer for the next input (i.e. next character and next timestep)\n",
    "        #lambda allows to specify the operation to be applied as a function on a tensor in a layer\n",
    "        #here it just takes a slice of x for a given timestep and returns it\n",
    "        x_t = Lambda(lambda x_t: X[:,t,:])(X)\n",
    "        \n",
    "        # reshapes x to be (1, vocab_size) \n",
    "        x_t = reshape_layer(x_t)\n",
    "        \n",
    "        #one step of LSTM to update hidden state and cell state\n",
    "        a, _, c = LSTM_cell(x_t, initial_state=[c, a])\n",
    "        \n",
    "        #apply softmax to a function of hidden state to get prediction\n",
    "        outputs = prediction_layer(a)\n",
    "        \n",
    "        outputs.append(output)\n",
    "        \n",
    "    model = Model(inputs= [X,a0,c0], outputs= outputs)\n",
    "    \n",
    "    return model\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def our model\n",
    "model = LSTM_model(max_len = max_len , n_a = n_a, vocab_size = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 58, 74)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 74)        0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_5[0][0]                   \n",
      "                                                                 lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "                                                                 lambda_8[0][0]                   \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "                                                                 lambda_11[0][0]                  \n",
      "                                                                 lambda_12[0][0]                  \n",
      "                                                                 lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "                                                                 lambda_17[0][0]                  \n",
      "                                                                 lambda_18[0][0]                  \n",
      "                                                                 lambda_19[0][0]                  \n",
      "                                                                 lambda_20[0][0]                  \n",
      "                                                                 lambda_21[0][0]                  \n",
      "                                                                 lambda_22[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 lambda_32[0][0]                  \n",
      "                                                                 lambda_33[0][0]                  \n",
      "                                                                 lambda_34[0][0]                  \n",
      "                                                                 lambda_35[0][0]                  \n",
      "                                                                 lambda_36[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "                                                                 lambda_38[0][0]                  \n",
      "                                                                 lambda_39[0][0]                  \n",
      "                                                                 lambda_40[0][0]                  \n",
      "                                                                 lambda_41[0][0]                  \n",
      "                                                                 lambda_42[0][0]                  \n",
      "                                                                 lambda_43[0][0]                  \n",
      "                                                                 lambda_44[0][0]                  \n",
      "                                                                 lambda_45[0][0]                  \n",
      "                                                                 lambda_46[0][0]                  \n",
      "                                                                 lambda_47[0][0]                  \n",
      "                                                                 lambda_48[0][0]                  \n",
      "                                                                 lambda_49[0][0]                  \n",
      "                                                                 lambda_50[0][0]                  \n",
      "                                                                 lambda_51[0][0]                  \n",
      "                                                                 lambda_52[0][0]                  \n",
      "                                                                 lambda_53[0][0]                  \n",
      "                                                                 lambda_54[0][0]                  \n",
      "                                                                 lambda_55[0][0]                  \n",
      "                                                                 lambda_56[0][0]                  \n",
      "                                                                 lambda_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "a0 (InputLayer)                 [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 70), (None,  40600       reshape[0][0]                    \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 a0[0][0]                         \n",
      "                                                                 reshape[1][0]                    \n",
      "                                                                 lstm[0][2]                       \n",
      "                                                                 lstm[0][0]                       \n",
      "                                                                 reshape[2][0]                    \n",
      "                                                                 lstm[1][2]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 reshape[3][0]                    \n",
      "                                                                 lstm[2][2]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 reshape[4][0]                    \n",
      "                                                                 lstm[3][2]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 reshape[5][0]                    \n",
      "                                                                 lstm[4][2]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 reshape[6][0]                    \n",
      "                                                                 lstm[5][2]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 reshape[7][0]                    \n",
      "                                                                 lstm[6][2]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 reshape[8][0]                    \n",
      "                                                                 lstm[7][2]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 reshape[9][0]                    \n",
      "                                                                 lstm[8][2]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 reshape[10][0]                   \n",
      "                                                                 lstm[9][2]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 reshape[11][0]                   \n",
      "                                                                 lstm[10][2]                      \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 reshape[12][0]                   \n",
      "                                                                 lstm[11][2]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 reshape[13][0]                   \n",
      "                                                                 lstm[12][2]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 reshape[14][0]                   \n",
      "                                                                 lstm[13][2]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 reshape[15][0]                   \n",
      "                                                                 lstm[14][2]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 reshape[16][0]                   \n",
      "                                                                 lstm[15][2]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 reshape[17][0]                   \n",
      "                                                                 lstm[16][2]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 reshape[18][0]                   \n",
      "                                                                 lstm[17][2]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 reshape[19][0]                   \n",
      "                                                                 lstm[18][2]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 reshape[20][0]                   \n",
      "                                                                 lstm[19][2]                      \n",
      "                                                                 lstm[19][0]                      \n",
      "                                                                 reshape[21][0]                   \n",
      "                                                                 lstm[20][2]                      \n",
      "                                                                 lstm[20][0]                      \n",
      "                                                                 reshape[22][0]                   \n",
      "                                                                 lstm[21][2]                      \n",
      "                                                                 lstm[21][0]                      \n",
      "                                                                 reshape[23][0]                   \n",
      "                                                                 lstm[22][2]                      \n",
      "                                                                 lstm[22][0]                      \n",
      "                                                                 reshape[24][0]                   \n",
      "                                                                 lstm[23][2]                      \n",
      "                                                                 lstm[23][0]                      \n",
      "                                                                 reshape[25][0]                   \n",
      "                                                                 lstm[24][2]                      \n",
      "                                                                 lstm[24][0]                      \n",
      "                                                                 reshape[26][0]                   \n",
      "                                                                 lstm[25][2]                      \n",
      "                                                                 lstm[25][0]                      \n",
      "                                                                 reshape[27][0]                   \n",
      "                                                                 lstm[26][2]                      \n",
      "                                                                 lstm[26][0]                      \n",
      "                                                                 reshape[28][0]                   \n",
      "                                                                 lstm[27][2]                      \n",
      "                                                                 lstm[27][0]                      \n",
      "                                                                 reshape[29][0]                   \n",
      "                                                                 lstm[28][2]                      \n",
      "                                                                 lstm[28][0]                      \n",
      "                                                                 reshape[30][0]                   \n",
      "                                                                 lstm[29][2]                      \n",
      "                                                                 lstm[29][0]                      \n",
      "                                                                 reshape[31][0]                   \n",
      "                                                                 lstm[30][2]                      \n",
      "                                                                 lstm[30][0]                      \n",
      "                                                                 reshape[32][0]                   \n",
      "                                                                 lstm[31][2]                      \n",
      "                                                                 lstm[31][0]                      \n",
      "                                                                 reshape[33][0]                   \n",
      "                                                                 lstm[32][2]                      \n",
      "                                                                 lstm[32][0]                      \n",
      "                                                                 reshape[34][0]                   \n",
      "                                                                 lstm[33][2]                      \n",
      "                                                                 lstm[33][0]                      \n",
      "                                                                 reshape[35][0]                   \n",
      "                                                                 lstm[34][2]                      \n",
      "                                                                 lstm[34][0]                      \n",
      "                                                                 reshape[36][0]                   \n",
      "                                                                 lstm[35][2]                      \n",
      "                                                                 lstm[35][0]                      \n",
      "                                                                 reshape[37][0]                   \n",
      "                                                                 lstm[36][2]                      \n",
      "                                                                 lstm[36][0]                      \n",
      "                                                                 reshape[38][0]                   \n",
      "                                                                 lstm[37][2]                      \n",
      "                                                                 lstm[37][0]                      \n",
      "                                                                 reshape[39][0]                   \n",
      "                                                                 lstm[38][2]                      \n",
      "                                                                 lstm[38][0]                      \n",
      "                                                                 reshape[40][0]                   \n",
      "                                                                 lstm[39][2]                      \n",
      "                                                                 lstm[39][0]                      \n",
      "                                                                 reshape[41][0]                   \n",
      "                                                                 lstm[40][2]                      \n",
      "                                                                 lstm[40][0]                      \n",
      "                                                                 reshape[42][0]                   \n",
      "                                                                 lstm[41][2]                      \n",
      "                                                                 lstm[41][0]                      \n",
      "                                                                 reshape[43][0]                   \n",
      "                                                                 lstm[42][2]                      \n",
      "                                                                 lstm[42][0]                      \n",
      "                                                                 reshape[44][0]                   \n",
      "                                                                 lstm[43][2]                      \n",
      "                                                                 lstm[43][0]                      \n",
      "                                                                 reshape[45][0]                   \n",
      "                                                                 lstm[44][2]                      \n",
      "                                                                 lstm[44][0]                      \n",
      "                                                                 reshape[46][0]                   \n",
      "                                                                 lstm[45][2]                      \n",
      "                                                                 lstm[45][0]                      \n",
      "                                                                 reshape[47][0]                   \n",
      "                                                                 lstm[46][2]                      \n",
      "                                                                 lstm[46][0]                      \n",
      "                                                                 reshape[48][0]                   \n",
      "                                                                 lstm[47][2]                      \n",
      "                                                                 lstm[47][0]                      \n",
      "                                                                 reshape[49][0]                   \n",
      "                                                                 lstm[48][2]                      \n",
      "                                                                 lstm[48][0]                      \n",
      "                                                                 reshape[50][0]                   \n",
      "                                                                 lstm[49][2]                      \n",
      "                                                                 lstm[49][0]                      \n",
      "                                                                 reshape[51][0]                   \n",
      "                                                                 lstm[50][2]                      \n",
      "                                                                 lstm[50][0]                      \n",
      "                                                                 reshape[52][0]                   \n",
      "                                                                 lstm[51][2]                      \n",
      "                                                                 lstm[51][0]                      \n",
      "                                                                 reshape[53][0]                   \n",
      "                                                                 lstm[52][2]                      \n",
      "                                                                 lstm[52][0]                      \n",
      "                                                                 reshape[54][0]                   \n",
      "                                                                 lstm[53][2]                      \n",
      "                                                                 lstm[53][0]                      \n",
      "                                                                 reshape[55][0]                   \n",
      "                                                                 lstm[54][2]                      \n",
      "                                                                 lstm[54][0]                      \n",
      "                                                                 reshape[56][0]                   \n",
      "                                                                 lstm[55][2]                      \n",
      "                                                                 lstm[55][0]                      \n",
      "                                                                 reshape[57][0]                   \n",
      "                                                                 lstm[56][2]                      \n",
      "                                                                 lstm[56][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_44 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_45 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_46 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_47 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_48 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_49 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_50 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_51 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_52 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_53 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_54 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_55 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_56 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_57 (Lambda)              (None, 74)           0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 74)           5254        lstm[0][0]                       \n",
      "                                                                 lstm[1][0]                       \n",
      "                                                                 lstm[2][0]                       \n",
      "                                                                 lstm[3][0]                       \n",
      "                                                                 lstm[4][0]                       \n",
      "                                                                 lstm[5][0]                       \n",
      "                                                                 lstm[6][0]                       \n",
      "                                                                 lstm[7][0]                       \n",
      "                                                                 lstm[8][0]                       \n",
      "                                                                 lstm[9][0]                       \n",
      "                                                                 lstm[10][0]                      \n",
      "                                                                 lstm[11][0]                      \n",
      "                                                                 lstm[12][0]                      \n",
      "                                                                 lstm[13][0]                      \n",
      "                                                                 lstm[14][0]                      \n",
      "                                                                 lstm[15][0]                      \n",
      "                                                                 lstm[16][0]                      \n",
      "                                                                 lstm[17][0]                      \n",
      "                                                                 lstm[18][0]                      \n",
      "                                                                 lstm[19][0]                      \n",
      "                                                                 lstm[20][0]                      \n",
      "                                                                 lstm[21][0]                      \n",
      "                                                                 lstm[22][0]                      \n",
      "                                                                 lstm[23][0]                      \n",
      "                                                                 lstm[24][0]                      \n",
      "                                                                 lstm[25][0]                      \n",
      "                                                                 lstm[26][0]                      \n",
      "                                                                 lstm[27][0]                      \n",
      "                                                                 lstm[28][0]                      \n",
      "                                                                 lstm[29][0]                      \n",
      "                                                                 lstm[30][0]                      \n",
      "                                                                 lstm[31][0]                      \n",
      "                                                                 lstm[32][0]                      \n",
      "                                                                 lstm[33][0]                      \n",
      "                                                                 lstm[34][0]                      \n",
      "                                                                 lstm[35][0]                      \n",
      "                                                                 lstm[36][0]                      \n",
      "                                                                 lstm[37][0]                      \n",
      "                                                                 lstm[38][0]                      \n",
      "                                                                 lstm[39][0]                      \n",
      "                                                                 lstm[40][0]                      \n",
      "                                                                 lstm[41][0]                      \n",
      "                                                                 lstm[42][0]                      \n",
      "                                                                 lstm[43][0]                      \n",
      "                                                                 lstm[44][0]                      \n",
      "                                                                 lstm[45][0]                      \n",
      "                                                                 lstm[46][0]                      \n",
      "                                                                 lstm[47][0]                      \n",
      "                                                                 lstm[48][0]                      \n",
      "                                                                 lstm[49][0]                      \n",
      "                                                                 lstm[50][0]                      \n",
      "                                                                 lstm[51][0]                      \n",
      "                                                                 lstm[52][0]                      \n",
      "                                                                 lstm[53][0]                      \n",
      "                                                                 lstm[54][0]                      \n",
      "                                                                 lstm[55][0]                      \n",
      "                                                                 lstm[56][0]                      \n",
      "                                                                 lstm[57][0]                      \n",
      "==================================================================================================\n",
      "Total params: 45,854\n",
      "Trainable params: 45,854\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "\n",
    "#optimizer = Adam(lr=0.01, beta_1=0.9, beta_2=0.9999, decay=0.001)\n",
    "optimizer = 'rmsprop'\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 23231\n",
    "a0 = np.zeros((k, n_a))\n",
    "c0 = np.zeros((k, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples: 23231\n",
      "Tx (length of sequence): 58\n",
      "total # of unique values: 74\n",
      "shape of X: (23231, 58, 74)\n",
      "Shape of Y: (58, 23231, 74)\n"
     ]
    }
   ],
   "source": [
    "len(list(Y))\n",
    "hg = list(Y)\n",
    "hg[0].shape\n",
    "\n",
    "print('number of training examples:', X.shape[0])\n",
    "print('Tx (length of sequence):', X.shape[1])\n",
    "print('total # of unique values:', vocab_size)\n",
    "print('shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)\n",
    "\n",
    "#58 items of shape (23231, 74)]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23231 samples\n",
      "Epoch 1/100\n",
      "23231/23231 [==============================] - 23s 973us/sample - loss: 142.0000 - dense_loss: 3.3714 - dense_1_loss: 3.0736 - dense_2_loss: 2.9934 - dense_3_loss: 2.9476 - dense_4_loss: 2.8822 - dense_5_loss: 2.8605 - dense_6_loss: 2.8988 - dense_7_loss: 2.8963 - dense_8_loss: 2.8973 - dense_9_loss: 2.8879 - dense_10_loss: 2.8597 - dense_11_loss: 2.8491 - dense_12_loss: 2.8305 - dense_13_loss: 2.7839 - dense_14_loss: 2.7821 - dense_15_loss: 2.7711 - dense_16_loss: 2.7784 - dense_17_loss: 2.7907 - dense_18_loss: 2.7976 - dense_19_loss: 2.8056 - dense_20_loss: 2.8039 - dense_21_loss: 2.8011 - dense_22_loss: 2.7984 - dense_23_loss: 2.7829 - dense_24_loss: 2.7891 - dense_25_loss: 2.7846 - dense_26_loss: 2.7803 - dense_27_loss: 2.7791 - dense_28_loss: 2.7665 - dense_29_loss: 2.7737 - dense_30_loss: 2.7460 - dense_31_loss: 2.7325 - dense_32_loss: 2.7053 - dense_33_loss: 2.6907 - dense_34_loss: 2.6662 - dense_35_loss: 2.6474 - dense_36_loss: 2.6149 - dense_37_loss: 2.5848 - dense_38_loss: 2.5620 - dense_39_loss: 2.5205 - dense_40_loss: 2.4683 - dense_41_loss: 2.4121 - dense_42_loss: 2.3461 - dense_43_loss: 2.2748 - dense_44_loss: 2.1878 - dense_45_loss: 2.1066 - dense_46_loss: 2.0318 - dense_47_loss: 1.9411 - dense_48_loss: 1.8443 - dense_49_loss: 1.7545 - dense_50_loss: 1.6515 - dense_51_loss: 1.5312 - dense_52_loss: 1.4062 - dense_53_loss: 1.2495 - dense_54_loss: 1.0568 - dense_55_loss: 0.8261 - dense_56_loss: 0.5345 - dense_57_loss: 0.2887 - dense_accuracy: 0.2090 - dense_1_accuracy: 0.1752 - dense_2_accuracy: 0.1806 - dense_3_accuracy: 0.1814 - dense_4_accuracy: 0.1929 - dense_5_accuracy: 0.2208 - dense_6_accuracy: 0.1944 - dense_7_accuracy: 0.1917 - dense_8_accuracy: 0.1743 - dense_9_accuracy: 0.1677 - dense_10_accuracy: 0.1720 - dense_11_accuracy: 0.1729 - dense_12_accuracy: 0.1772 - dense_13_accuracy: 0.1850 - dense_14_accuracy: 0.1933 - dense_15_accuracy: 0.2058 - dense_16_accuracy: 0.2076 - dense_17_accuracy: 0.2107 - dense_18_accuracy: 0.2108 - dense_19_accuracy: 0.2126 - dense_20_accuracy: 0.2120 - dense_21_accuracy: 0.2117 - dense_22_accuracy: 0.2049 - dense_23_accuracy: 0.2074 - dense_24_accuracy: 0.2030 - dense_25_accuracy: 0.2006 - dense_26_accuracy: 0.1996 - dense_27_accuracy: 0.1888 - dense_28_accuracy: 0.1952 - dense_29_accuracy: 0.1844 - dense_30_accuracy: 0.1896 - dense_31_accuracy: 0.1831 - dense_32_accuracy: 0.1819 - dense_33_accuracy: 0.1840 - dense_34_accuracy: 0.1785 - dense_35_accuracy: 0.1769 - dense_36_accuracy: 0.1688 - dense_37_accuracy: 0.1721 - dense_38_accuracy: 0.1666 - dense_39_accuracy: 0.1604 - dense_40_accuracy: 0.1686 - dense_41_accuracy: 0.1674 - dense_42_accuracy: 0.1701 - dense_43_accuracy: 0.1650 - dense_44_accuracy: 0.1648 - dense_45_accuracy: 0.1633 - dense_46_accuracy: 0.1507 - dense_47_accuracy: 0.1437 - dense_48_accuracy: 0.1293 - dense_49_accuracy: 0.1170 - dense_50_accuracy: 0.1059 - dense_51_accuracy: 0.0905 - dense_52_accuracy: 0.0741 - dense_53_accuracy: 0.0657 - dense_54_accuracy: 0.0603 - dense_55_accuracy: 0.0552 - dense_56_accuracy: 0.0585 - dense_57_accuracy: 0.0567\n",
      "Epoch 2/100\n",
      "23231/23231 [==============================] - 23s 1ms/sample - loss: 119.4595 - dense_loss: 2.4454 - dense_1_loss: 2.5604 - dense_2_loss: 2.5370 - dense_3_loss: 2.5543 - dense_4_loss: 2.4584 - dense_5_loss: 2.4312 - dense_6_loss: 2.4926 - dense_7_loss: 2.4830 - dense_8_loss: 2.4893 - dense_9_loss: 2.4755 - dense_10_loss: 2.4288 - dense_11_loss: 2.4105 - dense_12_loss: 2.3901 - dense_13_loss: 2.3322 - dense_14_loss: 2.3261 - dense_15_loss: 2.3254 - dense_16_loss: 2.3445 - dense_17_loss: 2.3540 - dense_18_loss: 2.3580 - dense_19_loss: 2.3653 - dense_20_loss: 2.3735 - dense_21_loss: 2.3747 - dense_22_loss: 2.3717 - dense_23_loss: 2.3649 - dense_24_loss: 2.3761 - dense_25_loss: 2.3758 - dense_26_loss: 2.3722 - dense_27_loss: 2.3846 - dense_28_loss: 2.3680 - dense_29_loss: 2.3773 - dense_30_loss: 2.3477 - dense_31_loss: 2.3448 - dense_32_loss: 2.3112 - dense_33_loss: 2.2889 - dense_34_loss: 2.2655 - dense_35_loss: 2.2442 - dense_36_loss: 2.2029 - dense_37_loss: 2.1759 - dense_38_loss: 2.1493 - dense_39_loss: 2.1194 - dense_40_loss: 2.0688 - dense_41_loss: 2.0197 - dense_42_loss: 1.9617 - dense_43_loss: 1.9001 - dense_44_loss: 1.8205 - dense_45_loss: 1.7604 - dense_46_loss: 1.7032 - dense_47_loss: 1.6213 - dense_48_loss: 1.5516 - dense_49_loss: 1.4763 - dense_50_loss: 1.3945 - dense_51_loss: 1.2900 - dense_52_loss: 1.1785 - dense_53_loss: 1.0352 - dense_54_loss: 0.8306 - dense_55_loss: 0.6120 - dense_56_loss: 0.3398 - dense_57_loss: 0.1445 - dense_accuracy: 0.3258 - dense_1_accuracy: 0.2255 - dense_2_accuracy: 0.2614 - dense_3_accuracy: 0.2517 - dense_4_accuracy: 0.2925 - dense_5_accuracy: 0.3016 - dense_6_accuracy: 0.2772 - dense_7_accuracy: 0.2713 - dense_8_accuracy: 0.2634 - dense_9_accuracy: 0.2608 - dense_10_accuracy: 0.2714 - dense_11_accuracy: 0.2756 - dense_12_accuracy: 0.2710 - dense_13_accuracy: 0.2853 - dense_14_accuracy: 0.2852 - dense_15_accuracy: 0.2888 - dense_16_accuracy: 0.2864 - dense_17_accuracy: 0.2871 - dense_18_accuracy: 0.2878 - dense_19_accuracy: 0.2881 - dense_20_accuracy: 0.2844 - dense_21_accuracy: 0.2833 - dense_22_accuracy: 0.2833 - dense_23_accuracy: 0.2862 - dense_24_accuracy: 0.2719 - dense_25_accuracy: 0.2723 - dense_26_accuracy: 0.2690 - dense_27_accuracy: 0.2617 - dense_28_accuracy: 0.2602 - dense_29_accuracy: 0.2553 - dense_30_accuracy: 0.2575 - dense_31_accuracy: 0.2572 - dense_32_accuracy: 0.2588 - dense_33_accuracy: 0.2620 - dense_34_accuracy: 0.2601 - dense_35_accuracy: 0.2643 - dense_36_accuracy: 0.2654 - dense_37_accuracy: 0.2662 - dense_38_accuracy: 0.2661 - dense_39_accuracy: 0.2550 - dense_40_accuracy: 0.2570 - dense_41_accuracy: 0.2575 - dense_42_accuracy: 0.2553 - dense_43_accuracy: 0.2483 - dense_44_accuracy: 0.2447 - dense_45_accuracy: 0.2355 - dense_46_accuracy: 0.2231 - dense_47_accuracy: 0.2152 - dense_48_accuracy: 0.1980 - dense_49_accuracy: 0.1845 - dense_50_accuracy: 0.1723 - dense_51_accuracy: 0.1575 - dense_52_accuracy: 0.1497 - dense_53_accuracy: 0.1411 - dense_54_accuracy: 0.1501 - dense_55_accuracy: 0.1468 - dense_56_accuracy: 0.1579 - dense_57_accuracy: 0.1198\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32/23231 [..............................] - ETA: 0s - loss: 117.4113 - dense_loss: 2.3999 - dense_1_loss: 2.4561 - dense_2_loss: 2.4520 - dense_3_loss: 2.6295 - dense_4_loss: 2.2153 - dense_5_loss: 2.5912 - dense_6_loss: 2.5457 - dense_7_loss: 2.4658 - dense_8_loss: 2.5306 - dense_9_loss: 2.3920 - dense_10_loss: 2.5064 - dense_11_loss: 2.0159 - dense_12_loss: 2.3868 - dense_13_loss: 2.3587 - dense_14_loss: 2.1908 - dense_15_loss: 2.2854 - dense_16_loss: 2.1767 - dense_17_loss: 1.9720 - dense_18_loss: 2.0414 - dense_19_loss: 2.2627 - dense_20_loss: 2.3033 - dense_21_loss: 2.2336 - dense_22_loss: 2.5918 - dense_23_loss: 2.2706 - dense_24_loss: 2.5676 - dense_25_loss: 2.2901 - dense_26_loss: 2.8251 - dense_27_loss: 2.4179 - dense_28_loss: 2.2150 - dense_29_loss: 2.1998 - dense_30_loss: 2.4292 - dense_31_loss: 2.2790 - dense_32_loss: 2.2147 - dense_33_loss: 2.1238 - dense_34_loss: 2.1943 - dense_35_loss: 2.2369 - dense_36_loss: 2.0886 - dense_37_loss: 1.9684 - dense_38_loss: 1.9521 - dense_39_loss: 1.8883 - dense_40_loss: 1.8725 - dense_41_loss: 2.4450 - dense_42_loss: 2.0529 - dense_43_loss: 1.9107 - dense_44_loss: 2.0356 - dense_45_loss: 1.7115 - dense_46_loss: 1.8574 - dense_47_loss: 1.9792 - dense_48_loss: 1.5996 - dense_49_loss: 1.5567 - dense_50_loss: 1.4251 - dense_51_loss: 1.1727 - dense_52_loss: 1.2558 - dense_53_loss: 0.8013 - dense_54_loss: 0.6513 - dense_55_loss: 0.5030 - dense_56_loss: 0.3333 - dense_57_loss: 0.0827 - dense_accuracy: 0.3438 - dense_1_accuracy: 0.3438 - dense_2_accuracy: 0.3125 - dense_3_accuracy: 0.1875 - dense_4_accuracy: 0.2812 - dense_5_accuracy: 0.2500 - dense_6_accuracy: 0.2812 - dense_7_accuracy: 0.3438 - dense_8_accuracy: 0.3125 - dense_9_accuracy: 0.2812 - dense_10_accuracy: 0.2812 - dense_11_accuracy: 0.3438 - dense_12_accuracy: 0.2188 - dense_13_accuracy: 0.2188 - dense_14_accuracy: 0.3750 - dense_15_accuracy: 0.3125 - dense_16_accuracy: 0.4062 - dense_17_accuracy: 0.5000 - dense_18_accuracy: 0.3750 - dense_19_accuracy: 0.4375 - dense_20_accuracy: 0.2812 - dense_21_accuracy: 0.3125 - dense_22_accuracy: 0.1875 - dense_23_accuracy: 0.3125 - dense_24_accuracy: 0.2188 - dense_25_accuracy: 0.3750 - dense_26_accuracy: 0.2188 - dense_27_accuracy: 0.2812 - dense_28_accuracy: 0.3750 - dense_29_accuracy: 0.3438 - dense_30_accuracy: 0.2500 - dense_31_accuracy: 0.2500 - dense_32_accuracy: 0.1875 - dense_33_accuracy: 0.3125 - dense_34_accuracy: 0.2812 - dense_35_accuracy: 0.2500 - dense_36_accuracy: 0.1875 - dense_37_accuracy: 0.4375 - dense_38_accuracy: 0.3438 - dense_39_accuracy: 0.3750 - dense_40_accuracy: 0.2812 - dense_41_accuracy: 0.2500 - dense_42_accuracy: 0.2188 - dense_43_accuracy: 0.4062 - dense_44_accuracy: 0.2500 - dense_45_accuracy: 0.2812 - dense_46_accuracy: 0.2188 - dense_47_accuracy: 0.2500 - dense_48_accuracy: 0.2500 - dense_49_accuracy: 0.2500 - dense_50_accuracy: 0.2500 - dense_51_accuracy: 0.2812 - dense_52_accuracy: 0.2188 - dense_53_accuracy: 0.2188 - dense_54_accuracy: 0.1875 - dense_55_accuracy: 0.1875 - dense_56_accuracy: 0.1875 - dense_57_accuracy: 0.0938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   96/23231 [..............................] - ETA: 18s - loss: 115.3394 - dense_loss: 2.3104 - dense_1_loss: 2.4111 - dense_2_loss: 2.3874 - dense_3_loss: 2.6600 - dense_4_loss: 2.2750 - dense_5_loss: 2.4966 - dense_6_loss: 2.4855 - dense_7_loss: 2.3305 - dense_8_loss: 2.4018 - dense_9_loss: 2.4615 - dense_10_loss: 2.4527 - dense_11_loss: 2.2623 - dense_12_loss: 2.1804 - dense_13_loss: 2.1732 - dense_14_loss: 2.3497 - dense_15_loss: 2.4058 - dense_16_loss: 2.2308 - dense_17_loss: 2.0869 - dense_18_loss: 2.1627 - dense_19_loss: 2.1615 - dense_20_loss: 2.1718 - dense_21_loss: 2.3015 - dense_22_loss: 2.3822 - dense_23_loss: 2.2592 - dense_24_loss: 2.4075 - dense_25_loss: 2.2716 - dense_26_loss: 2.5573 - dense_27_loss: 2.3425 - dense_28_loss: 2.2073 - dense_29_loss: 2.0596 - dense_30_loss: 2.1934 - dense_31_loss: 2.3996 - dense_32_loss: 2.1853 - dense_33_loss: 2.1718 - dense_34_loss: 2.2698 - dense_35_loss: 2.0269 - dense_36_loss: 2.3290 - dense_37_loss: 2.2286 - dense_38_loss: 1.8276 - dense_39_loss: 1.8191 - dense_40_loss: 1.9367 - dense_41_loss: 2.3455 - dense_42_loss: 1.8957 - dense_43_loss: 2.0992 - dense_44_loss: 1.9944 - dense_45_loss: 1.7965 - dense_46_loss: 1.7218 - dense_47_loss: 1.8227 - dense_48_loss: 1.5155 - dense_49_loss: 1.2714 - dense_50_loss: 1.2099 - dense_51_loss: 0.9913 - dense_52_loss: 1.0616 - dense_53_loss: 0.8637 - dense_54_loss: 0.6922 - dense_55_loss: 0.5572 - dense_56_loss: 0.3292 - dense_57_loss: 0.1376 - dense_accuracy: 0.3229 - dense_1_accuracy: 0.2604 - dense_2_accuracy: 0.3333 - dense_3_accuracy: 0.2396 - dense_4_accuracy: 0.3021 - dense_5_accuracy: 0.2708 - dense_6_accuracy: 0.2708 - dense_7_accuracy: 0.3646 - dense_8_accuracy: 0.3229 - dense_9_accuracy: 0.2708 - dense_10_accuracy: 0.2604 - dense_11_accuracy: 0.3229 - dense_12_accuracy: 0.3021 - dense_13_accuracy: 0.3333 - dense_14_accuracy: 0.3125 - dense_15_accuracy: 0.3021 - dense_16_accuracy: 0.3646 - dense_17_accuracy: 0.3854 - dense_18_accuracy: 0.3958 - dense_19_accuracy: 0.4062 - dense_20_accuracy: 0.2917 - dense_21_accuracy: 0.3021 - dense_22_accuracy: 0.2604 - dense_23_accuracy: 0.2812 - dense_24_accuracy: 0.2708 - dense_25_accuracy: 0.3750 - dense_26_accuracy: 0.2292 - dense_27_accuracy: 0.2708 - dense_28_accuracy: 0.3229 - dense_29_accuracy: 0.3229 - dense_30_accuracy: 0.3021 - dense_31_accuracy: 0.2188 - dense_32_accuracy: 0.2812 - dense_33_accuracy: 0.2917 - dense_34_accuracy: 0.2604 - dense_35_accuracy: 0.3125 - dense_36_accuracy: 0.2188 - dense_37_accuracy: 0.2812 - dense_38_accuracy: 0.3854 - dense_39_accuracy: 0.3750 - dense_40_accuracy: 0.2708 - dense_41_accuracy: 0.2292 - dense_42_accuracy: 0.2812 - dense_43_accuracy: 0.2708 - dense_44_accuracy: 0.2188 - dense_45_accuracy: 0.2396 - dense_46_accuracy: 0.3333 - dense_47_accuracy: 0.1979 - dense_48_accuracy: 0.2292 - dense_49_accuracy: 0.3229 - dense_50_accuracy: 0.2292 - dense_51_accuracy: 0.2292 - dense_52_accuracy: 0.1250 - dense_53_accuracy: 0.1458 - dense_54_accuracy: 0.1667 - dense_55_accuracy: 0.1146 - dense_56_accuracy: 0.1979 - dense_57_accuracy: 0.1146"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23231/23231 [==============================] - 27s 1ms/sample - loss: 112.1267 - dense_loss: 2.1913 - dense_1_loss: 2.4180 - dense_2_loss: 2.3830 - dense_3_loss: 2.4313 - dense_4_loss: 2.3357 - dense_5_loss: 2.3074 - dense_6_loss: 2.3643 - dense_7_loss: 2.3561 - dense_8_loss: 2.3514 - dense_9_loss: 2.3329 - dense_10_loss: 2.2769 - dense_11_loss: 2.2553 - dense_12_loss: 2.2244 - dense_13_loss: 2.1661 - dense_14_loss: 2.1519 - dense_15_loss: 2.1503 - dense_16_loss: 2.1865 - dense_17_loss: 2.1980 - dense_18_loss: 2.2095 - dense_19_loss: 2.2218 - dense_20_loss: 2.2293 - dense_21_loss: 2.2378 - dense_22_loss: 2.2299 - dense_23_loss: 2.2241 - dense_24_loss: 2.2356 - dense_25_loss: 2.2355 - dense_26_loss: 2.2352 - dense_27_loss: 2.2458 - dense_28_loss: 2.2324 - dense_29_loss: 2.2419 - dense_30_loss: 2.2140 - dense_31_loss: 2.2174 - dense_32_loss: 2.1795 - dense_33_loss: 2.1580 - dense_34_loss: 2.1353 - dense_35_loss: 2.1071 - dense_36_loss: 2.0604 - dense_37_loss: 2.0315 - dense_38_loss: 1.9982 - dense_39_loss: 1.9662 - dense_40_loss: 1.9241 - dense_41_loss: 1.8859 - dense_42_loss: 1.8362 - dense_43_loss: 1.7783 - dense_44_loss: 1.7035 - dense_45_loss: 1.6570 - dense_46_loss: 1.6014 - dense_47_loss: 1.5279 - dense_48_loss: 1.4628 - dense_49_loss: 1.3919 - dense_50_loss: 1.3217 - dense_51_loss: 1.2249 - dense_52_loss: 1.1181 - dense_53_loss: 0.9799 - dense_54_loss: 0.7756 - dense_55_loss: 0.5592 - dense_56_loss: 0.3081 - dense_57_loss: 0.1427 - dense_accuracy: 0.3469 - dense_1_accuracy: 0.2854 - dense_2_accuracy: 0.3119 - dense_3_accuracy: 0.2897 - dense_4_accuracy: 0.3248 - dense_5_accuracy: 0.3364 - dense_6_accuracy: 0.3151 - dense_7_accuracy: 0.3054 - dense_8_accuracy: 0.3008 - dense_9_accuracy: 0.3019 - dense_10_accuracy: 0.3186 - dense_11_accuracy: 0.3226 - dense_12_accuracy: 0.3207 - dense_13_accuracy: 0.3344 - dense_14_accuracy: 0.3361 - dense_15_accuracy: 0.3377 - dense_16_accuracy: 0.3303 - dense_17_accuracy: 0.3331 - dense_18_accuracy: 0.3286 - dense_19_accuracy: 0.3262 - dense_20_accuracy: 0.3217 - dense_21_accuracy: 0.3223 - dense_22_accuracy: 0.3227 - dense_23_accuracy: 0.3240 - dense_24_accuracy: 0.3130 - dense_25_accuracy: 0.3134 - dense_26_accuracy: 0.3093 - dense_27_accuracy: 0.3009 - dense_28_accuracy: 0.2989 - dense_29_accuracy: 0.2942 - dense_30_accuracy: 0.2971 - dense_31_accuracy: 0.2916 - dense_32_accuracy: 0.2974 - dense_33_accuracy: 0.2961 - dense_34_accuracy: 0.2934 - dense_35_accuracy: 0.2948 - dense_36_accuracy: 0.3029 - dense_37_accuracy: 0.3041 - dense_38_accuracy: 0.3020 - dense_39_accuracy: 0.2990 - dense_40_accuracy: 0.2967 - dense_41_accuracy: 0.2956 - dense_42_accuracy: 0.2892 - dense_43_accuracy: 0.2793 - dense_44_accuracy: 0.2723 - dense_45_accuracy: 0.2627 - dense_46_accuracy: 0.2482 - dense_47_accuracy: 0.2377 - dense_48_accuracy: 0.2222 - dense_49_accuracy: 0.2049 - dense_50_accuracy: 0.1913 - dense_51_accuracy: 0.1733 - dense_52_accuracy: 0.1629 - dense_53_accuracy: 0.1549 - dense_54_accuracy: 0.1614 - dense_55_accuracy: 0.1618 - dense_56_accuracy: 0.1632 - dense_57_accuracy: 0.1169\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3168/23231 [===>..........................] - ETA: 21s - loss: 109.9913 - dense_loss: 2.1236 - dense_1_loss: 2.3498 - dense_2_loss: 2.3230 - dense_3_loss: 2.3424 - dense_4_loss: 2.3290 - dense_5_loss: 2.2500 - dense_6_loss: 2.2875 - dense_7_loss: 2.3164 - dense_8_loss: 2.3194 - dense_9_loss: 2.2857 - dense_10_loss: 2.2014 - dense_11_loss: 2.2086 - dense_12_loss: 2.1293 - dense_13_loss: 2.1001 - dense_14_loss: 2.0915 - dense_15_loss: 2.0811 - dense_16_loss: 2.1173 - dense_17_loss: 2.1439 - dense_18_loss: 2.1684 - dense_19_loss: 2.1916 - dense_20_loss: 2.2025 - dense_21_loss: 2.2123 - dense_22_loss: 2.1725 - dense_23_loss: 2.1843 - dense_24_loss: 2.1966 - dense_25_loss: 2.1887 - dense_26_loss: 2.1509 - dense_27_loss: 2.2195 - dense_28_loss: 2.1978 - dense_29_loss: 2.1671 - dense_30_loss: 2.1641 - dense_31_loss: 2.1966 - dense_32_loss: 2.1023 - dense_33_loss: 2.0991 - dense_34_loss: 2.0993 - dense_35_loss: 2.0600 - dense_36_loss: 2.0090 - dense_37_loss: 1.9934 - dense_38_loss: 1.9874 - dense_39_loss: 1.9103 - dense_40_loss: 1.9187 - dense_41_loss: 1.8351 - dense_42_loss: 1.8398 - dense_43_loss: 1.7777 - dense_44_loss: 1.7188 - dense_45_loss: 1.6295 - dense_46_loss: 1.5915 - dense_47_loss: 1.5046 - dense_48_loss: 1.4730 - dense_49_loss: 1.3679 - dense_50_loss: 1.3484 - dense_51_loss: 1.2559 - dense_52_loss: 1.0981 - dense_53_loss: 0.9697 - dense_54_loss: 0.7580 - dense_55_loss: 0.5694 - dense_56_loss: 0.3187 - dense_57_loss: 0.1428 - dense_accuracy: 0.3695 - dense_1_accuracy: 0.3083 - dense_2_accuracy: 0.3241 - dense_3_accuracy: 0.3257 - dense_4_accuracy: 0.3251 - dense_5_accuracy: 0.3421 - dense_6_accuracy: 0.3257 - dense_7_accuracy: 0.3206 - dense_8_accuracy: 0.3189 - dense_9_accuracy: 0.3083 - dense_10_accuracy: 0.3405 - dense_11_accuracy: 0.3305 - dense_12_accuracy: 0.3392 - dense_13_accuracy: 0.3512 - dense_14_accuracy: 0.3608 - dense_15_accuracy: 0.3589 - dense_16_accuracy: 0.3454 - dense_17_accuracy: 0.3450 - dense_18_accuracy: 0.3296 - dense_19_accuracy: 0.3305 - dense_20_accuracy: 0.3264 - dense_21_accuracy: 0.3276 - dense_22_accuracy: 0.3396 - dense_23_accuracy: 0.3338 - dense_24_accuracy: 0.3138 - dense_25_accuracy: 0.3231 - dense_26_accuracy: 0.3254 - dense_27_accuracy: 0.3173 - dense_28_accuracy: 0.3038 - dense_29_accuracy: 0.3180 - dense_30_accuracy: 0.3138 - dense_31_accuracy: 0.3051 - dense_32_accuracy: 0.3305 - dense_33_accuracy: 0.3189 - dense_34_accuracy: 0.3061 - dense_35_accuracy: 0.3093 - dense_36_accuracy: 0.3244 - dense_37_accuracy: 0.3102 - dense_38_accuracy: 0.3106 - dense_39_accuracy: 0.3212 - dense_40_accuracy: 0.3035 - dense_41_accuracy: 0.3177 - dense_42_accuracy: 0.2993 - dense_43_accuracy: 0.2986 - dense_44_accuracy: 0.2825 - dense_45_accuracy: 0.2690 - dense_46_accuracy: 0.2606 - dense_47_accuracy: 0.2452 - dense_48_accuracy: 0.2223 - dense_49_accuracy: 0.2171 - dense_50_accuracy: 0.2001 - dense_51_accuracy: 0.1714 - dense_52_accuracy: 0.1608 - dense_53_accuracy: 0.1601 - dense_54_accuracy: 0.1759 - dense_55_accuracy: 0.1637 - dense_56_accuracy: 0.1698 - dense_57_accuracy: 0.12 - ETA: 21s - loss: 109.9198 - dense_loss: 2.1247 - dense_1_loss: 2.3453 - dense_2_loss: 2.3198 - dense_3_loss: 2.3489 - dense_4_loss: 2.3248 - dense_5_loss: 2.2537 - dense_6_loss: 2.2935 - dense_7_loss: 2.3098 - dense_8_loss: 2.3163 - dense_9_loss: 2.2857 - dense_10_loss: 2.2062 - dense_11_loss: 2.2100 - dense_12_loss: 2.1341 - dense_13_loss: 2.1016 - dense_14_loss: 2.0946 - dense_15_loss: 2.0855 - dense_16_loss: 2.1213 - dense_17_loss: 2.1442 - dense_18_loss: 2.1660 - dense_19_loss: 2.1928 - dense_20_loss: 2.2060 - dense_21_loss: 2.2074 - dense_22_loss: 2.1735 - dense_23_loss: 2.1878 - dense_24_loss: 2.1993 - dense_25_loss: 2.1879 - dense_26_loss: 2.1474 - dense_27_loss: 2.2199 - dense_28_loss: 2.1940 - dense_29_loss: 2.1668 - dense_30_loss: 2.1658 - dense_31_loss: 2.1979 - dense_32_loss: 2.0962 - dense_33_loss: 2.0969 - dense_34_loss: 2.0960 - dense_35_loss: 2.0587 - dense_36_loss: 2.0033 - dense_37_loss: 1.9901 - dense_38_loss: 1.9852 - dense_39_loss: 1.9096 - dense_40_loss: 1.9164 - dense_41_loss: 1.8302 - dense_42_loss: 1.8359 - dense_43_loss: 1.7720 - dense_44_loss: 1.7182 - dense_45_loss: 1.6249 - dense_46_loss: 1.5849 - dense_47_loss: 1.4983 - dense_48_loss: 1.4694 - dense_49_loss: 1.3663 - dense_50_loss: 1.3439 - dense_51_loss: 1.2513 - dense_52_loss: 1.0934 - dense_53_loss: 0.9663 - dense_54_loss: 0.7565 - dense_55_loss: 0.5658 - dense_56_loss: 0.3162 - dense_57_loss: 0.1412 - dense_accuracy: 0.3693 - dense_1_accuracy: 0.3103 - dense_2_accuracy: 0.3235 - dense_3_accuracy: 0.3229 - dense_4_accuracy: 0.3254 - dense_5_accuracy: 0.3396 - dense_6_accuracy: 0.3229 - dense_7_accuracy: 0.3220 - dense_8_accuracy: 0.3204 - dense_9_accuracy: 0.3090 - dense_10_accuracy: 0.3393 - dense_11_accuracy: 0.3302 - dense_12_accuracy: 0.3384 - dense_13_accuracy: 0.3516 - dense_14_accuracy: 0.3617 - dense_15_accuracy: 0.3586 - dense_16_accuracy: 0.3456 - dense_17_accuracy: 0.3447 - dense_18_accuracy: 0.3295 - dense_19_accuracy: 0.3308 - dense_20_accuracy: 0.3258 - dense_21_accuracy: 0.3280 - dense_22_accuracy: 0.3387 - dense_23_accuracy: 0.3336 - dense_24_accuracy: 0.3138 - dense_25_accuracy: 0.3223 - dense_26_accuracy: 0.3261 - dense_27_accuracy: 0.3169 - dense_28_accuracy: 0.3040 - dense_29_accuracy: 0.3176 - dense_30_accuracy: 0.3138 - dense_31_accuracy: 0.3049 - dense_32_accuracy: 0.3321 - dense_33_accuracy: 0.3179 - dense_34_accuracy: 0.3046 - dense_35_accuracy: 0.3081 - dense_36_accuracy: 0.3248 - dense_37_accuracy: 0.3100 - dense_38_accuracy: 0.3087 - dense_39_accuracy: 0.3188 - dense_40_accuracy: 0.3021 - dense_41_accuracy: 0.3150 - dense_42_accuracy: 0.2983 - dense_43_accuracy: 0.2973 - dense_44_accuracy: 0.2809 - dense_45_accuracy: 0.2683 - dense_46_accuracy: 0.2601 - dense_47_accuracy: 0.2459 - dense_48_accuracy: 0.2210 - dense_49_accuracy: 0.2156 - dense_50_accuracy: 0.1989 - dense_51_accuracy: 0.1708 - dense_52_accuracy: 0.1616 - dense_53_accuracy: 0.1600 - dense_54_accuracy: 0.1752 - dense_55_accuracy: 0.1622 - dense_56_accuracy: 0.1673 - dense_57_accuracy: 0.1190\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 3232/23231 [===>..........................] - ETA: 21s - loss: 109.8896 - dense_loss: 2.1233 - dense_1_loss: 2.3433 - dense_2_loss: 2.3221 - dense_3_loss: 2.3498 - dense_4_loss: 2.3208 - dense_5_loss: 2.2554 - dense_6_loss: 2.2906 - dense_7_loss: 2.3118 - dense_8_loss: 2.3192 - dense_9_loss: 2.2859 - dense_10_loss: 2.2053 - dense_11_loss: 2.2087 - dense_12_loss: 2.1318 - dense_13_loss: 2.1065 - dense_14_loss: 2.0934 - dense_15_loss: 2.0823 - dense_16_loss: 2.1207 - dense_17_loss: 2.1421 - dense_18_loss: 2.1611 - dense_19_loss: 2.1952 - dense_20_loss: 2.2081 - dense_21_loss: 2.2088 - dense_22_loss: 2.1765 - dense_23_loss: 2.1862 - dense_24_loss: 2.1993 - dense_25_loss: 2.1867 - dense_26_loss: 2.1441 - dense_27_loss: 2.2202 - dense_28_loss: 2.1925 - dense_29_loss: 2.1617 - dense_30_loss: 2.1679 - dense_31_loss: 2.1926 - dense_32_loss: 2.1014 - dense_33_loss: 2.0975 - dense_34_loss: 2.0983 - dense_35_loss: 2.0568 - dense_36_loss: 2.0020 - dense_37_loss: 2.0010 - dense_38_loss: 1.9806 - dense_39_loss: 1.9109 - dense_40_loss: 1.9204 - dense_41_loss: 1.8313 - dense_42_loss: 1.8385 - dense_43_loss: 1.7693 - dense_44_loss: 1.7143 - dense_45_loss: 1.6229 - dense_46_loss: 1.5791 - dense_47_loss: 1.4953 - dense_48_loss: 1.4658 - dense_49_loss: 1.3645 - dense_50_loss: 1.3410 - dense_51_loss: 1.2478 - dense_52_loss: 1.0928 - dense_53_loss: 0.9650 - dense_54_loss: 0.7564 - dense_55_loss: 0.5665 - dense_56_loss: 0.3144 - dense_57_loss: 0.1420 - dense_accuracy: 0.3688 - dense_1_accuracy: 0.3113 - dense_2_accuracy: 0.3215 - dense_3_accuracy: 0.3221 - dense_4_accuracy: 0.3261 - dense_5_accuracy: 0.3397 - dense_6_accuracy: 0.3236 - dense_7_accuracy: 0.3209 - dense_8_accuracy: 0.3187 - dense_9_accuracy: 0.3079 - dense_10_accuracy: 0.3394 - dense_11_accuracy: 0.3304 - dense_12_accuracy: 0.3385 - dense_13_accuracy: 0.3524 - dense_14_accuracy: 0.3611 - dense_15_accuracy: 0.3601 - dense_16_accuracy: 0.3468 - dense_17_accuracy: 0.3456 - dense_18_accuracy: 0.3317 - dense_19_accuracy: 0.3301 - dense_20_accuracy: 0.3252 - dense_21_accuracy: 0.3280 - dense_22_accuracy: 0.3388 - dense_23_accuracy: 0.3354 - dense_24_accuracy: 0.3144 - dense_25_accuracy: 0.3221 - dense_26_accuracy: 0.3274 - dense_27_accuracy: 0.3171 - dense_28_accuracy: 0.3045 - dense_29_accuracy: 0.3202 - dense_30_accuracy: 0.3147 - dense_31_accuracy: 0.3072 - dense_32_accuracy: 0.3298 - dense_33_accuracy: 0.3184 - dense_34_accuracy: 0.3060 - dense_35_accuracy: 0.3085 - dense_36_accuracy: 0.3252 - dense_37_accuracy: 0.3085 - dense_38_accuracy: 0.3100 - dense_39_accuracy: 0.3202 - dense_40_accuracy: 0.3029 - dense_41_accuracy: 0.3140 - dense_42_accuracy: 0.2970 - dense_43_accuracy: 0.2989 - dense_44_accuracy: 0.2806 - dense_45_accuracy: 0.2673 - dense_46_accuracy: 0.2596 - dense_47_accuracy: 0.2447 - dense_48_accuracy: 0.2200 - dense_49_accuracy: 0.2150 - dense_50_accuracy: 0.1977 - dense_51_accuracy: 0.1692 - dense_52_accuracy: 0.1612 - dense_53_accuracy: 0.1606 - dense_54_accuracy: 0.1748 - dense_55_accuracy: 0.1621 - dense_56_accuracy: 0.1674 - dense_57_accuracy: 0.1197"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23231/23231 [==============================] - 26s 1ms/sample - loss: 107.7532 - dense_loss: 2.0781 - dense_1_loss: 2.3214 - dense_2_loss: 2.2737 - dense_3_loss: 2.3436 - dense_4_loss: 2.2563 - dense_5_loss: 2.2248 - dense_6_loss: 2.2824 - dense_7_loss: 2.2760 - dense_8_loss: 2.2654 - dense_9_loss: 2.2419 - dense_10_loss: 2.1828 - dense_11_loss: 2.1668 - dense_12_loss: 2.1313 - dense_13_loss: 2.0700 - dense_14_loss: 2.0562 - dense_15_loss: 2.0561 - dense_16_loss: 2.0978 - dense_17_loss: 2.1125 - dense_18_loss: 2.1305 - dense_19_loss: 2.1414 - dense_20_loss: 2.1511 - dense_21_loss: 2.1578 - dense_22_loss: 2.1471 - dense_23_loss: 2.1432 - dense_24_loss: 2.1543 - dense_25_loss: 2.1529 - dense_26_loss: 2.1552 - dense_27_loss: 2.1577 - dense_28_loss: 2.1476 - dense_29_loss: 2.1565 - dense_30_loss: 2.1279 - dense_31_loss: 2.1321 - dense_32_loss: 2.0925 - dense_33_loss: 2.0757 - dense_34_loss: 2.0483 - dense_35_loss: 2.0220 - dense_36_loss: 1.9746 - dense_37_loss: 1.9510 - dense_38_loss: 1.9149 - dense_39_loss: 1.8850 - dense_40_loss: 1.8450 - dense_41_loss: 1.8097 - dense_42_loss: 1.7642 - dense_43_loss: 1.7045 - dense_44_loss: 1.6364 - dense_45_loss: 1.5925 - dense_46_loss: 1.5412 - dense_47_loss: 1.4714 - dense_48_loss: 1.4068 - dense_49_loss: 1.3431 - dense_50_loss: 1.2759 - dense_51_loss: 1.1816 - dense_52_loss: 1.0790 - dense_53_loss: 0.9399 - dense_54_loss: 0.7374 - dense_55_loss: 0.5325 - dense_56_loss: 0.2963 - dense_57_loss: 0.1388 - dense_accuracy: 0.3618 - dense_1_accuracy: 0.3244 - dense_2_accuracy: 0.3431 - dense_3_accuracy: 0.3410 - dense_4_accuracy: 0.3466 - dense_5_accuracy: 0.3545 - dense_6_accuracy: 0.3308 - dense_7_accuracy: 0.3287 - dense_8_accuracy: 0.3250 - dense_9_accuracy: 0.3260 - dense_10_accuracy: 0.3414 - dense_11_accuracy: 0.3451 - dense_12_accuracy: 0.3445 - dense_13_accuracy: 0.3646 - dense_14_accuracy: 0.3671 - dense_15_accuracy: 0.3695 - dense_16_accuracy: 0.3625 - dense_17_accuracy: 0.3569 - dense_18_accuracy: 0.3516 - dense_19_accuracy: 0.3498 - dense_20_accuracy: 0.3431 - dense_21_accuracy: 0.3421 - dense_22_accuracy: 0.3478 - dense_23_accuracy: 0.3468 - dense_24_accuracy: 0.3322 - dense_25_accuracy: 0.3346 - dense_26_accuracy: 0.3290 - dense_27_accuracy: 0.3269 - dense_28_accuracy: 0.3227 - dense_29_accuracy: 0.3196 - dense_30_accuracy: 0.3226 - dense_31_accuracy: 0.3168 - dense_32_accuracy: 0.3237 - dense_33_accuracy: 0.3180 - dense_34_accuracy: 0.3176 - dense_35_accuracy: 0.3179 - dense_36_accuracy: 0.3266 - dense_37_accuracy: 0.3234 - dense_38_accuracy: 0.3230 - dense_39_accuracy: 0.3224 - dense_40_accuracy: 0.3167 - dense_41_accuracy: 0.3157 - dense_42_accuracy: 0.3067 - dense_43_accuracy: 0.2969 - dense_44_accuracy: 0.2919 - dense_45_accuracy: 0.2752 - dense_46_accuracy: 0.2612 - dense_47_accuracy: 0.2500 - dense_48_accuracy: 0.2351 - dense_49_accuracy: 0.2145 - dense_50_accuracy: 0.2011 - dense_51_accuracy: 0.1807 - dense_52_accuracy: 0.1691 - dense_53_accuracy: 0.1601 - dense_54_accuracy: 0.1660 - dense_55_accuracy: 0.1643 - dense_56_accuracy: 0.1604 - dense_57_accuracy: 0.1127\n",
      "Epoch 5/100\n",
      "23231/23231 [==============================] - 26s 1ms/sample - loss: 104.6756 - dense_loss: 2.0160 - dense_1_loss: 2.2485 - dense_2_loss: 2.1835 - dense_3_loss: 2.2710 - dense_4_loss: 2.1887 - dense_5_loss: 2.1685 - dense_6_loss: 2.2225 - dense_7_loss: 2.2186 - dense_8_loss: 2.2029 - dense_9_loss: 2.1792 - dense_10_loss: 2.1160 - dense_11_loss: 2.1003 - dense_12_loss: 2.0663 - dense_13_loss: 2.0068 - dense_14_loss: 1.9916 - dense_15_loss: 1.9919 - dense_16_loss: 2.0401 - dense_17_loss: 2.0559 - dense_18_loss: 2.0786 - dense_19_loss: 2.0841 - dense_20_loss: 2.0950 - dense_21_loss: 2.1022 - dense_22_loss: 2.0917 - dense_23_loss: 2.0863 - dense_24_loss: 2.0960 - dense_25_loss: 2.0979 - dense_26_loss: 2.0968 - dense_27_loss: 2.0982 - dense_28_loss: 2.0865 - dense_29_loss: 2.0964 - dense_30_loss: 2.0658 - dense_31_loss: 2.0725 - dense_32_loss: 2.0310 - dense_33_loss: 2.0145 - dense_34_loss: 1.9893 - dense_35_loss: 1.9637 - dense_36_loss: 1.9136 - dense_37_loss: 1.8921 - dense_38_loss: 1.8587 - dense_39_loss: 1.8297 - dense_40_loss: 1.7928 - dense_41_loss: 1.7581 - dense_42_loss: 1.7134 - dense_43_loss: 1.6535 - dense_44_loss: 1.5913 - dense_45_loss: 1.5481 - dense_46_loss: 1.4994 - dense_47_loss: 1.4332 - dense_48_loss: 1.3696 - dense_49_loss: 1.3073 - dense_50_loss: 1.2434 - dense_51_loss: 1.1511 - dense_52_loss: 1.0490 - dense_53_loss: 0.9119 - dense_54_loss: 0.7120 - dense_55_loss: 0.5082 - dense_56_loss: 0.2880 - dense_57_loss: 0.1362 - dense_accuracy: 0.3842 - dense_1_accuracy: 0.3414 - dense_2_accuracy: 0.3707 - dense_3_accuracy: 0.3678 - dense_4_accuracy: 0.3657 - dense_5_accuracy: 0.3693 - dense_6_accuracy: 0.3413 - dense_7_accuracy: 0.3379 - dense_8_accuracy: 0.3381 - dense_9_accuracy: 0.3393 - dense_10_accuracy: 0.3578 - dense_11_accuracy: 0.3622 - dense_12_accuracy: 0.3665 - dense_13_accuracy: 0.3858 - dense_14_accuracy: 0.3906 - dense_15_accuracy: 0.3951 - dense_16_accuracy: 0.3833 - dense_17_accuracy: 0.3762 - dense_18_accuracy: 0.3693 - dense_19_accuracy: 0.3660 - dense_20_accuracy: 0.3603 - dense_21_accuracy: 0.3566 - dense_22_accuracy: 0.3577 - dense_23_accuracy: 0.3594 - dense_24_accuracy: 0.3492 - dense_25_accuracy: 0.3476 - dense_26_accuracy: 0.3432 - dense_27_accuracy: 0.3389 - dense_28_accuracy: 0.3367 - dense_29_accuracy: 0.3344 - dense_30_accuracy: 0.3376 - dense_31_accuracy: 0.3351 - dense_32_accuracy: 0.3394 - dense_33_accuracy: 0.3338 - dense_34_accuracy: 0.3346 - dense_35_accuracy: 0.3323 - dense_36_accuracy: 0.3430 - dense_37_accuracy: 0.3399 - dense_38_accuracy: 0.3407 - dense_39_accuracy: 0.3398 - dense_40_accuracy: 0.3321 - dense_41_accuracy: 0.3292 - dense_42_accuracy: 0.3218 - dense_43_accuracy: 0.3114 - dense_44_accuracy: 0.3030 - dense_45_accuracy: 0.2865 - dense_46_accuracy: 0.2717 - dense_47_accuracy: 0.2593 - dense_48_accuracy: 0.2449 - dense_49_accuracy: 0.2213 - dense_50_accuracy: 0.2068 - dense_51_accuracy: 0.1880 - dense_52_accuracy: 0.1728 - dense_53_accuracy: 0.1648 - dense_54_accuracy: 0.1690 - dense_55_accuracy: 0.1668 - dense_56_accuracy: 0.1610 - dense_57_accuracy: 0.1103\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32/23231 [..............................] - ETA: 0s - loss: 100.0319 - dense_loss: 2.3822 - dense_1_loss: 2.3714 - dense_2_loss: 2.1001 - dense_3_loss: 2.1190 - dense_4_loss: 2.1406 - dense_5_loss: 2.2490 - dense_6_loss: 2.1481 - dense_7_loss: 2.2291 - dense_8_loss: 1.9782 - dense_9_loss: 2.5114 - dense_10_loss: 2.0432 - dense_11_loss: 1.3233 - dense_12_loss: 2.0789 - dense_13_loss: 2.1188 - dense_14_loss: 2.1280 - dense_15_loss: 1.7183 - dense_16_loss: 2.4715 - dense_17_loss: 1.9661 - dense_18_loss: 1.9350 - dense_19_loss: 1.6864 - dense_20_loss: 2.4029 - dense_21_loss: 2.2944 - dense_22_loss: 2.0321 - dense_23_loss: 1.7777 - dense_24_loss: 2.1005 - dense_25_loss: 2.2477 - dense_26_loss: 1.7702 - dense_27_loss: 2.4723 - dense_28_loss: 2.1589 - dense_29_loss: 1.9855 - dense_30_loss: 2.0840 - dense_31_loss: 1.7819 - dense_32_loss: 2.0415 - dense_33_loss: 1.6229 - dense_34_loss: 2.0281 - dense_35_loss: 1.6751 - dense_36_loss: 1.6453 - dense_37_loss: 1.6432 - dense_38_loss: 1.8900 - dense_39_loss: 1.8025 - dense_40_loss: 1.5576 - dense_41_loss: 1.4646 - dense_42_loss: 1.8412 - dense_43_loss: 1.6625 - dense_44_loss: 1.5081 - dense_45_loss: 1.4615 - dense_46_loss: 1.3033 - dense_47_loss: 1.4598 - dense_48_loss: 0.9702 - dense_49_loss: 1.0983 - dense_50_loss: 1.1961 - dense_51_loss: 0.7405 - dense_52_loss: 0.7938 - dense_53_loss: 0.6802 - dense_54_loss: 0.4826 - dense_55_loss: 0.4327 - dense_56_loss: 0.1547 - dense_57_loss: 0.0691 - dense_accuracy: 0.3125 - dense_1_accuracy: 0.1562 - dense_2_accuracy: 0.4062 - dense_3_accuracy: 0.5000 - dense_4_accuracy: 0.4062 - dense_5_accuracy: 0.2812 - dense_6_accuracy: 0.4062 - dense_7_accuracy: 0.3125 - dense_8_accuracy: 0.4062 - dense_9_accuracy: 0.2812 - dense_10_accuracy: 0.3125 - dense_11_accuracy: 0.6250 - dense_12_accuracy: 0.3438 - dense_13_accuracy: 0.3750 - dense_14_accuracy: 0.3438 - dense_15_accuracy: 0.4688 - dense_16_accuracy: 0.2500 - dense_17_accuracy: 0.3125 - dense_18_accuracy: 0.4062 - dense_19_accuracy: 0.5625 - dense_20_accuracy: 0.2500 - dense_21_accuracy: 0.2500 - dense_22_accuracy: 0.3750 - dense_23_accuracy: 0.4688 - dense_24_accuracy: 0.3438 - dense_25_accuracy: 0.3438 - dense_26_accuracy: 0.3438 - dense_27_accuracy: 0.1562 - dense_28_accuracy: 0.2812 - dense_29_accuracy: 0.3125 - dense_30_accuracy: 0.3438 - dense_31_accuracy: 0.4375 - dense_32_accuracy: 0.2812 - dense_33_accuracy: 0.4688 - dense_34_accuracy: 0.3125 - dense_35_accuracy: 0.4688 - dense_36_accuracy: 0.5000 - dense_37_accuracy: 0.5000 - dense_38_accuracy: 0.3750 - dense_39_accuracy: 0.4688 - dense_40_accuracy: 0.4688 - dense_41_accuracy: 0.3750 - dense_42_accuracy: 0.3438 - dense_43_accuracy: 0.2812 - dense_44_accuracy: 0.3125 - dense_45_accuracy: 0.1875 - dense_46_accuracy: 0.2500 - dense_47_accuracy: 0.1875 - dense_48_accuracy: 0.1875 - dense_49_accuracy: 0.1875 - dense_50_accuracy: 0.1250 - dense_51_accuracy: 0.2188 - dense_52_accuracy: 0.1875 - dense_53_accuracy: 0.0938 - dense_54_accuracy: 0.0625 - dense_55_accuracy: 0.1562 - dense_56_accuracy: 0.0625 - dense_57_accuracy: 0.0625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   96/23231 [..............................] - ETA: 19s - loss: 101.9473 - dense_loss: 1.9682 - dense_1_loss: 2.2694 - dense_2_loss: 2.1328 - dense_3_loss: 2.1730 - dense_4_loss: 2.1537 - dense_5_loss: 2.1008 - dense_6_loss: 2.1408 - dense_7_loss: 2.1354 - dense_8_loss: 2.1944 - dense_9_loss: 2.3080 - dense_10_loss: 2.0236 - dense_11_loss: 1.7697 - dense_12_loss: 2.0002 - dense_13_loss: 1.9143 - dense_14_loss: 2.0250 - dense_15_loss: 1.8282 - dense_16_loss: 2.0712 - dense_17_loss: 1.8767 - dense_18_loss: 1.9405 - dense_19_loss: 2.0199 - dense_20_loss: 2.0113 - dense_21_loss: 2.1462 - dense_22_loss: 1.9602 - dense_23_loss: 2.0573 - dense_24_loss: 2.0451 - dense_25_loss: 2.1475 - dense_26_loss: 1.8954 - dense_27_loss: 2.0014 - dense_28_loss: 2.3138 - dense_29_loss: 1.8768 - dense_30_loss: 1.9489 - dense_31_loss: 1.9508 - dense_32_loss: 2.2659 - dense_33_loss: 1.8452 - dense_34_loss: 2.0597 - dense_35_loss: 1.9803 - dense_36_loss: 1.9356 - dense_37_loss: 1.9808 - dense_38_loss: 1.8783 - dense_39_loss: 1.9192 - dense_40_loss: 1.7443 - dense_41_loss: 1.8668 - dense_42_loss: 1.7538 - dense_43_loss: 1.5923 - dense_44_loss: 1.5355 - dense_45_loss: 1.4514 - dense_46_loss: 1.2497 - dense_47_loss: 1.4846 - dense_48_loss: 1.1580 - dense_49_loss: 1.2878 - dense_50_loss: 1.1660 - dense_51_loss: 1.0967 - dense_52_loss: 1.0809 - dense_53_loss: 0.7275 - dense_54_loss: 0.6175 - dense_55_loss: 0.4107 - dense_56_loss: 0.3098 - dense_57_loss: 0.1489 - dense_accuracy: 0.4167 - dense_1_accuracy: 0.3125 - dense_2_accuracy: 0.3854 - dense_3_accuracy: 0.3958 - dense_4_accuracy: 0.4271 - dense_5_accuracy: 0.3646 - dense_6_accuracy: 0.3750 - dense_7_accuracy: 0.3021 - dense_8_accuracy: 0.3750 - dense_9_accuracy: 0.2604 - dense_10_accuracy: 0.3646 - dense_11_accuracy: 0.4583 - dense_12_accuracy: 0.3229 - dense_13_accuracy: 0.4062 - dense_14_accuracy: 0.3750 - dense_15_accuracy: 0.4167 - dense_16_accuracy: 0.3333 - dense_17_accuracy: 0.3854 - dense_18_accuracy: 0.4167 - dense_19_accuracy: 0.3646 - dense_20_accuracy: 0.3438 - dense_21_accuracy: 0.3750 - dense_22_accuracy: 0.3958 - dense_23_accuracy: 0.3333 - dense_24_accuracy: 0.3646 - dense_25_accuracy: 0.3229 - dense_26_accuracy: 0.3750 - dense_27_accuracy: 0.3542 - dense_28_accuracy: 0.2500 - dense_29_accuracy: 0.3646 - dense_30_accuracy: 0.4062 - dense_31_accuracy: 0.4479 - dense_32_accuracy: 0.2812 - dense_33_accuracy: 0.3750 - dense_34_accuracy: 0.3229 - dense_35_accuracy: 0.3750 - dense_36_accuracy: 0.3750 - dense_37_accuracy: 0.3958 - dense_38_accuracy: 0.3854 - dense_39_accuracy: 0.3750 - dense_40_accuracy: 0.4062 - dense_41_accuracy: 0.3125 - dense_42_accuracy: 0.3333 - dense_43_accuracy: 0.3229 - dense_44_accuracy: 0.3229 - dense_45_accuracy: 0.3125 - dense_46_accuracy: 0.3125 - dense_47_accuracy: 0.2292 - dense_48_accuracy: 0.2396 - dense_49_accuracy: 0.1771 - dense_50_accuracy: 0.2188 - dense_51_accuracy: 0.1875 - dense_52_accuracy: 0.1875 - dense_53_accuracy: 0.1562 - dense_54_accuracy: 0.1354 - dense_55_accuracy: 0.1771 - dense_56_accuracy: 0.1042 - dense_57_accuracy: 0.0938"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23231/23231 [==============================] - 25s 1ms/sample - loss: 102.3246 - dense_loss: 1.9782 - dense_1_loss: 2.1898 - dense_2_loss: 2.1043 - dense_3_loss: 2.2141 - dense_4_loss: 2.1352 - dense_5_loss: 2.1240 - dense_6_loss: 2.1770 - dense_7_loss: 2.1716 - dense_8_loss: 2.1571 - dense_9_loss: 2.1287 - dense_10_loss: 2.0648 - dense_11_loss: 2.0538 - dense_12_loss: 2.0202 - dense_13_loss: 1.9572 - dense_14_loss: 1.9461 - dense_15_loss: 1.9482 - dense_16_loss: 1.9952 - dense_17_loss: 2.0140 - dense_18_loss: 2.0345 - dense_19_loss: 2.0422 - dense_20_loss: 2.0518 - dense_21_loss: 2.0594 - dense_22_loss: 2.0475 - dense_23_loss: 2.0414 - dense_24_loss: 2.0532 - dense_25_loss: 2.0545 - dense_26_loss: 2.0545 - dense_27_loss: 2.0499 - dense_28_loss: 2.0422 - dense_29_loss: 2.0483 - dense_30_loss: 2.0195 - dense_31_loss: 2.0264 - dense_32_loss: 1.9869 - dense_33_loss: 1.9695 - dense_34_loss: 1.9431 - dense_35_loss: 1.9189 - dense_36_loss: 1.8689 - dense_37_loss: 1.8505 - dense_38_loss: 1.8162 - dense_39_loss: 1.7878 - dense_40_loss: 1.7523 - dense_41_loss: 1.7188 - dense_42_loss: 1.6785 - dense_43_loss: 1.6186 - dense_44_loss: 1.5584 - dense_45_loss: 1.5141 - dense_46_loss: 1.4676 - dense_47_loss: 1.4031 - dense_48_loss: 1.3391 - dense_49_loss: 1.2812 - dense_50_loss: 1.2163 - dense_51_loss: 1.1240 - dense_52_loss: 1.0239 - dense_53_loss: 0.8896 - dense_54_loss: 0.6888 - dense_55_loss: 0.4904 - dense_56_loss: 0.2782 - dense_57_loss: 0.1345 - dense_accuracy: 0.3928 - dense_1_accuracy: 0.3663 - dense_2_accuracy: 0.3957 - dense_3_accuracy: 0.3836 - dense_4_accuracy: 0.3881 - dense_5_accuracy: 0.3820 - dense_6_accuracy: 0.3553 - dense_7_accuracy: 0.3488 - dense_8_accuracy: 0.3519 - dense_9_accuracy: 0.3568 - dense_10_accuracy: 0.3732 - dense_11_accuracy: 0.3733 - dense_12_accuracy: 0.3804 - dense_13_accuracy: 0.4030 - dense_14_accuracy: 0.4069 - dense_15_accuracy: 0.4138 - dense_16_accuracy: 0.3992 - dense_17_accuracy: 0.3919 - dense_18_accuracy: 0.3828 - dense_19_accuracy: 0.3780 - dense_20_accuracy: 0.3740 - dense_21_accuracy: 0.3690 - dense_22_accuracy: 0.3712 - dense_23_accuracy: 0.3701 - dense_24_accuracy: 0.3629 - dense_25_accuracy: 0.3614 - dense_26_accuracy: 0.3551 - dense_27_accuracy: 0.3545 - dense_28_accuracy: 0.3518 - dense_29_accuracy: 0.3481 - dense_30_accuracy: 0.3493 - dense_31_accuracy: 0.3510 - dense_32_accuracy: 0.3538 - dense_33_accuracy: 0.3487 - dense_34_accuracy: 0.3494 - dense_35_accuracy: 0.3466 - dense_36_accuracy: 0.3566 - dense_37_accuracy: 0.3525 - dense_38_accuracy: 0.3525 - dense_39_accuracy: 0.3492 - dense_40_accuracy: 0.3460 - dense_41_accuracy: 0.3401 - dense_42_accuracy: 0.3315 - dense_43_accuracy: 0.3206 - dense_44_accuracy: 0.3119 - dense_45_accuracy: 0.2963 - dense_46_accuracy: 0.2795 - dense_47_accuracy: 0.2671 - dense_48_accuracy: 0.2519 - dense_49_accuracy: 0.2280 - dense_50_accuracy: 0.2111 - dense_51_accuracy: 0.1950 - dense_52_accuracy: 0.1779 - dense_53_accuracy: 0.1677 - dense_54_accuracy: 0.1730 - dense_55_accuracy: 0.1701 - dense_56_accuracy: 0.1630 - dense_57_accuracy: 0.10919s - loss: 102.6742 - dense_loss: 1.9929 - dense_1_loss: 2.2020 - dense_2_loss: 2.1297 - dense_3_loss: 2.2227 - dense_4_loss: 2.1466 - dense_5_loss: 2.1247 - dense_6_loss: 2.1894 - dense_7_loss: 2.1801 - dense_8_loss: 2.1640 - dense_9_loss: 2.1350 - dense_10_loss: 2.0737 - dense_11_loss: 2.0609 - dense_12_loss: 2.0236 - dense_13_loss: 1.9605 - dense_14_loss: 1.9552 - dense_15_loss: 1.9597 - dense_16_loss: 2.0117 - dense_17_loss: 2.0224 - dense_18_loss: 2.0378 - dense_19_loss: 2.0466 - dense_20_loss: 2.0589 - dense_21_loss: 2.0651 - dense_22_loss: 2.0524 - dense_23_loss: 2.0453 - dense_24_loss: 2.0687 - dense_25_loss: 2.0514 - dense_26_loss: 2.0581 - dense_27_loss: 2.0518 - dense_28_loss: 2.0341 - dense_29_loss: 2.0548 - dense_30_loss: 2.0248 - dense_31_loss: 2.0366 - dense_32_loss: 1.9948 - dense_33_loss: 1.9590 - dense_34_loss: 1.9492 - dense_35_loss: 1.9159 - dense_36_loss: 1.8766 - dense_37_loss: 1.8598 - dense_38_loss: 1.8334 - dense_39_loss: 1.7901 - dense_40_loss: 1.7675 - dense_41_loss: 1.7337 - dense_42_loss: 1.6906 - dense_43_loss: 1.6328 - dense_44_loss: 1.5653 - dense_45_loss: 1.5258 - dense_46_loss: 1.4746 - dense_47_loss: 1.4133 - dense_48_loss: 1.3375 - dense_49_loss: 1.2894 - dense_50_loss: 1.2158 - dense_51_loss: 1.1145 - dense_52_loss: 1.0292 - dense_53_loss: 0.8848 - dense_54_loss: 0.6847 - dense_55_loss: 0.4826 - dense_56_loss: 0.2781 - dense_57_loss: 0.1336 - dense_accuracy: 0.3918 - dense_1_accuracy: 0.3613 - dense_2_accuracy: 0.3896 - dense_3_accuracy: 0.3828 - dense_4_accuracy: 0.3848 - dense_5_accuracy: 0.3821 - dense_6_accuracy: 0.3502 - dense_7_accuracy: 0.3450 - dense_8_accuracy: 0.3494 - dense_9_accuracy: 0.3530 - dense_10_accuracy: 0.3705 - dense_11_accuracy: 0.3708 - dense_12_accuracy: 0.3786 - dense_13_accuracy: 0.4015 - dense_14_accuracy: 0.4032 - dense_15_accuracy: 0.4088 - dense_16_accuracy: 0.3915 - dense_17_accuracy: 0.3891 - dense_18_accuracy: 0.3831 - dense_19_accuracy: 0.3781 - dense_20_accuracy: 0.3739 - dense_21_accuracy: 0.3665 - dense_22_accuracy: 0.3712 - dense_23_accuracy: 0.3684 - dense_24_accuracy: 0.3591 - dense_25_accuracy: 0.3613 - dense_26_accuracy: 0.3550 - dense_27_accuracy: 0.3539 - dense_28_accuracy: 0.3565 - dense_29_accuracy: 0.3476 - dense_30_accuracy: 0.3494 - dense_31_accuracy: 0.3503 - dense_32_accuracy: 0.3515 - dense_33_accuracy: 0.3533 - dense_34_accuracy: 0.3444 - dense_35_accuracy: 0.3480 - dense_36_accuracy: 0.3539 - dense_37_accuracy: 0.3497 - dense_38_accuracy: 0.3498 - dense_39_accuracy: 0.3494 - dense_40_accuracy: 0.3422 - dense_41_accuracy: 0.3383 - dense_42_accuracy: 0.3308 - dense_43_accuracy: 0.3167 - dense_44_accuracy: 0.3122 - dense_45_accuracy: 0.2973 - dense_46_accuracy: 0.2805 - dense_47_accuracy: 0.2656 - dense_48_accuracy: 0.2549 - dense_49_accuracy: 0.2262 - dense_50_accuracy: 0.2110 - dense_51_accuracy: 0.1981 - dense_52_accuracy: 0.1761 - de\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32/23231 [..............................] - ETA: 0s - loss: 100.2913 - dense_loss: 1.8454 - dense_1_loss: 1.9413 - dense_2_loss: 2.1540 - dense_3_loss: 2.5455 - dense_4_loss: 1.7996 - dense_5_loss: 1.8304 - dense_6_loss: 2.0854 - dense_7_loss: 1.9390 - dense_8_loss: 2.1358 - dense_9_loss: 1.9083 - dense_10_loss: 2.1026 - dense_11_loss: 1.5915 - dense_12_loss: 1.8110 - dense_13_loss: 2.3918 - dense_14_loss: 2.3246 - dense_15_loss: 1.7673 - dense_16_loss: 1.7855 - dense_17_loss: 1.6422 - dense_18_loss: 2.0761 - dense_19_loss: 1.6992 - dense_20_loss: 2.0304 - dense_21_loss: 2.2137 - dense_22_loss: 1.9635 - dense_23_loss: 1.9040 - dense_24_loss: 2.1155 - dense_25_loss: 2.0028 - dense_26_loss: 2.4245 - dense_27_loss: 2.2875 - dense_28_loss: 1.7061 - dense_29_loss: 2.1244 - dense_30_loss: 1.8143 - dense_31_loss: 1.6691 - dense_32_loss: 1.9101 - dense_33_loss: 2.0628 - dense_34_loss: 1.6040 - dense_35_loss: 2.3769 - dense_36_loss: 1.9485 - dense_37_loss: 2.1080 - dense_38_loss: 1.8480 - dense_39_loss: 1.3583 - dense_40_loss: 1.8967 - dense_41_loss: 1.6655 - dense_42_loss: 1.6052 - dense_43_loss: 1.8758 - dense_44_loss: 1.6422 - dense_45_loss: 1.6361 - dense_46_loss: 1.7906 - dense_47_loss: 1.4423 - dense_48_loss: 1.4101 - dense_49_loss: 1.5044 - dense_50_loss: 1.0176 - dense_51_loss: 1.2114 - dense_52_loss: 0.8575 - dense_53_loss: 0.8653 - dense_54_loss: 0.4595 - dense_55_loss: 0.2079 - dense_56_loss: 0.2558 - dense_57_loss: 0.0986 - dense_accuracy: 0.4062 - dense_1_accuracy: 0.4688 - dense_2_accuracy: 0.4062 - dense_3_accuracy: 0.3438 - dense_4_accuracy: 0.5312 - dense_5_accuracy: 0.5312 - dense_6_accuracy: 0.4688 - dense_7_accuracy: 0.4688 - dense_8_accuracy: 0.2812 - dense_9_accuracy: 0.4062 - dense_10_accuracy: 0.2812 - dense_11_accuracy: 0.5000 - dense_12_accuracy: 0.4375 - dense_13_accuracy: 0.2812 - dense_14_accuracy: 0.2500 - dense_15_accuracy: 0.4688 - dense_16_accuracy: 0.4062 - dense_17_accuracy: 0.5625 - dense_18_accuracy: 0.3438 - dense_19_accuracy: 0.4375 - dense_20_accuracy: 0.4688 - dense_21_accuracy: 0.3438 - dense_22_accuracy: 0.3750 - dense_23_accuracy: 0.4375 - dense_24_accuracy: 0.4062 - dense_25_accuracy: 0.2812 - dense_26_accuracy: 0.3750 - dense_27_accuracy: 0.2500 - dense_28_accuracy: 0.5625 - dense_29_accuracy: 0.3438 - dense_30_accuracy: 0.3125 - dense_31_accuracy: 0.4688 - dense_32_accuracy: 0.3438 - dense_33_accuracy: 0.3125 - dense_34_accuracy: 0.4688 - dense_35_accuracy: 0.2188 - dense_36_accuracy: 0.3438 - dense_37_accuracy: 0.2500 - dense_38_accuracy: 0.4062 - dense_39_accuracy: 0.5625 - dense_40_accuracy: 0.4688 - dense_41_accuracy: 0.4375 - dense_42_accuracy: 0.3750 - dense_43_accuracy: 0.4062 - dense_44_accuracy: 0.4375 - dense_45_accuracy: 0.4062 - dense_46_accuracy: 0.2812 - dense_47_accuracy: 0.2812 - dense_48_accuracy: 0.3750 - dense_49_accuracy: 0.2812 - dense_50_accuracy: 0.3438 - dense_51_accuracy: 0.1562 - dense_52_accuracy: 0.0938 - dense_53_accuracy: 0.1875 - dense_54_accuracy: 0.2500 - dense_55_accuracy: 0.2188 - dense_56_accuracy: 0.2500 - dense_57_accuracy: 0.0625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   96/23231 [..............................] - ETA: 16s - loss: 100.4647 - dense_loss: 2.1912 - dense_1_loss: 2.1998 - dense_2_loss: 2.0923 - dense_3_loss: 2.2458 - dense_4_loss: 2.1202 - dense_5_loss: 2.0692 - dense_6_loss: 2.0394 - dense_7_loss: 2.1690 - dense_8_loss: 2.3006 - dense_9_loss: 1.9883 - dense_10_loss: 2.0005 - dense_11_loss: 1.9207 - dense_12_loss: 1.8810 - dense_13_loss: 2.0818 - dense_14_loss: 2.2273 - dense_15_loss: 1.6972 - dense_16_loss: 1.8904 - dense_17_loss: 1.7928 - dense_18_loss: 1.9207 - dense_19_loss: 2.0216 - dense_20_loss: 2.0567 - dense_21_loss: 2.2166 - dense_22_loss: 2.1664 - dense_23_loss: 1.9948 - dense_24_loss: 2.2270 - dense_25_loss: 2.0762 - dense_26_loss: 1.9836 - dense_27_loss: 2.0673 - dense_28_loss: 2.0745 - dense_29_loss: 1.9829 - dense_30_loss: 1.9053 - dense_31_loss: 1.6338 - dense_32_loss: 1.9073 - dense_33_loss: 2.0622 - dense_34_loss: 1.9609 - dense_35_loss: 1.9692 - dense_36_loss: 1.7940 - dense_37_loss: 1.7582 - dense_38_loss: 1.7480 - dense_39_loss: 1.5641 - dense_40_loss: 1.5888 - dense_41_loss: 1.7241 - dense_42_loss: 1.6634 - dense_43_loss: 1.6761 - dense_44_loss: 1.6722 - dense_45_loss: 1.6175 - dense_46_loss: 1.4864 - dense_47_loss: 1.3561 - dense_48_loss: 1.1818 - dense_49_loss: 1.2332 - dense_50_loss: 1.0397 - dense_51_loss: 1.0824 - dense_52_loss: 1.0214 - dense_53_loss: 0.8194 - dense_54_loss: 0.5622 - dense_55_loss: 0.4009 - dense_56_loss: 0.1914 - dense_57_loss: 0.1486 - dense_accuracy: 0.3229 - dense_1_accuracy: 0.3750 - dense_2_accuracy: 0.4062 - dense_3_accuracy: 0.4062 - dense_4_accuracy: 0.4375 - dense_5_accuracy: 0.4375 - dense_6_accuracy: 0.4062 - dense_7_accuracy: 0.4167 - dense_8_accuracy: 0.2708 - dense_9_accuracy: 0.3542 - dense_10_accuracy: 0.3750 - dense_11_accuracy: 0.4167 - dense_12_accuracy: 0.4271 - dense_13_accuracy: 0.4167 - dense_14_accuracy: 0.3646 - dense_15_accuracy: 0.4583 - dense_16_accuracy: 0.3854 - dense_17_accuracy: 0.4583 - dense_18_accuracy: 0.4167 - dense_19_accuracy: 0.3438 - dense_20_accuracy: 0.4167 - dense_21_accuracy: 0.3646 - dense_22_accuracy: 0.3750 - dense_23_accuracy: 0.3542 - dense_24_accuracy: 0.3125 - dense_25_accuracy: 0.3542 - dense_26_accuracy: 0.4167 - dense_27_accuracy: 0.3958 - dense_28_accuracy: 0.4792 - dense_29_accuracy: 0.3333 - dense_30_accuracy: 0.3542 - dense_31_accuracy: 0.4479 - dense_32_accuracy: 0.3333 - dense_33_accuracy: 0.3333 - dense_34_accuracy: 0.3438 - dense_35_accuracy: 0.3646 - dense_36_accuracy: 0.3750 - dense_37_accuracy: 0.4271 - dense_38_accuracy: 0.4271 - dense_39_accuracy: 0.4896 - dense_40_accuracy: 0.3958 - dense_41_accuracy: 0.3229 - dense_42_accuracy: 0.3646 - dense_43_accuracy: 0.3229 - dense_44_accuracy: 0.3854 - dense_45_accuracy: 0.2917 - dense_46_accuracy: 0.3229 - dense_47_accuracy: 0.2292 - dense_48_accuracy: 0.3438 - dense_49_accuracy: 0.2708 - dense_50_accuracy: 0.2500 - dense_51_accuracy: 0.1562 - dense_52_accuracy: 0.1354 - dense_53_accuracy: 0.1771 - dense_54_accuracy: 0.2083 - dense_55_accuracy: 0.1875 - dense_56_accuracy: 0.1979 - dense_57_accuracy: 0.0625"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23231/23231 [==============================] - 24s 1ms/sample - loss: 100.4585 - dense_loss: 1.9532 - dense_1_loss: 2.1408 - dense_2_loss: 2.0428 - dense_3_loss: 2.1633 - dense_4_loss: 2.0923 - dense_5_loss: 2.0901 - dense_6_loss: 2.1376 - dense_7_loss: 2.1312 - dense_8_loss: 2.1187 - dense_9_loss: 2.0899 - dense_10_loss: 2.0257 - dense_11_loss: 2.0148 - dense_12_loss: 1.9826 - dense_13_loss: 1.9202 - dense_14_loss: 1.9074 - dense_15_loss: 1.9104 - dense_16_loss: 1.9583 - dense_17_loss: 1.9773 - dense_18_loss: 1.9995 - dense_19_loss: 2.0069 - dense_20_loss: 2.0183 - dense_21_loss: 2.0236 - dense_22_loss: 2.0156 - dense_23_loss: 2.0067 - dense_24_loss: 2.0200 - dense_25_loss: 2.0206 - dense_26_loss: 2.0215 - dense_27_loss: 2.0133 - dense_28_loss: 2.0064 - dense_29_loss: 2.0138 - dense_30_loss: 1.9806 - dense_31_loss: 1.9917 - dense_32_loss: 1.9492 - dense_33_loss: 1.9311 - dense_34_loss: 1.9049 - dense_35_loss: 1.8846 - dense_36_loss: 1.8339 - dense_37_loss: 1.8157 - dense_38_loss: 1.7826 - dense_39_loss: 1.7587 - dense_40_loss: 1.7215 - dense_41_loss: 1.6888 - dense_42_loss: 1.6468 - dense_43_loss: 1.5918 - dense_44_loss: 1.5299 - dense_45_loss: 1.4886 - dense_46_loss: 1.4432 - dense_47_loss: 1.3807 - dense_48_loss: 1.3175 - dense_49_loss: 1.2591 - dense_50_loss: 1.1967 - dense_51_loss: 1.1042 - dense_52_loss: 1.0058 - dense_53_loss: 0.8707 - dense_54_loss: 0.6737 - dense_55_loss: 0.4769 - dense_56_loss: 0.2751 - dense_57_loss: 0.1313 - dense_accuracy: 0.3924 - dense_1_accuracy: 0.3766 - dense_2_accuracy: 0.4180 - dense_3_accuracy: 0.3988 - dense_4_accuracy: 0.3985 - dense_5_accuracy: 0.3922 - dense_6_accuracy: 0.3699 - dense_7_accuracy: 0.3616 - dense_8_accuracy: 0.3617 - dense_9_accuracy: 0.3696 - dense_10_accuracy: 0.3880 - dense_11_accuracy: 0.3851 - dense_12_accuracy: 0.3917 - dense_13_accuracy: 0.4162 - dense_14_accuracy: 0.4224 - dense_15_accuracy: 0.4279 - dense_16_accuracy: 0.4133 - dense_17_accuracy: 0.4068 - dense_18_accuracy: 0.3953 - dense_19_accuracy: 0.3902 - dense_20_accuracy: 0.3836 - dense_21_accuracy: 0.3804 - dense_22_accuracy: 0.3804 - dense_23_accuracy: 0.3797 - dense_24_accuracy: 0.3707 - dense_25_accuracy: 0.3695 - dense_26_accuracy: 0.3625 - dense_27_accuracy: 0.3615 - dense_28_accuracy: 0.3621 - dense_29_accuracy: 0.3617 - dense_30_accuracy: 0.3628 - dense_31_accuracy: 0.3581 - dense_32_accuracy: 0.3622 - dense_33_accuracy: 0.3612 - dense_34_accuracy: 0.3597 - dense_35_accuracy: 0.3544 - dense_36_accuracy: 0.3680 - dense_37_accuracy: 0.3625 - dense_38_accuracy: 0.3628 - dense_39_accuracy: 0.3584 - dense_40_accuracy: 0.3532 - dense_41_accuracy: 0.3474 - dense_42_accuracy: 0.3414 - dense_43_accuracy: 0.3271 - dense_44_accuracy: 0.3208 - dense_45_accuracy: 0.3043 - dense_46_accuracy: 0.2854 - dense_47_accuracy: 0.2717 - dense_48_accuracy: 0.2587 - dense_49_accuracy: 0.2339 - dense_50_accuracy: 0.2196 - dense_51_accuracy: 0.2002 - dense_52_accuracy: 0.1846 - dense_53_accuracy: 0.1735 - dense_54_accuracy: 0.1807 - dense_55_accuracy: 0.1733 - dense_56_accuracy: 0.1619 - dense_57_accuracy: 0.1081\n",
      "Epoch 8/100\n",
      "23231/23231 [==============================] - 25s 1ms/sample - loss: 98.8988 - dense_loss: 1.9352 - dense_1_loss: 2.0987 - dense_2_loss: 1.9885 - dense_3_loss: 2.1190 - dense_4_loss: 2.0544 - dense_5_loss: 2.0608 - dense_6_loss: 2.1042 - dense_7_loss: 2.0960 - dense_8_loss: 2.0887 - dense_9_loss: 2.0556 - dense_10_loss: 1.9925 - dense_11_loss: 1.9793 - dense_12_loss: 1.9540 - dense_13_loss: 1.8919 - dense_14_loss: 1.8793 - dense_15_loss: 1.8788 - dense_16_loss: 1.9294 - dense_17_loss: 1.9478 - dense_18_loss: 1.9710 - dense_19_loss: 1.9775 - dense_20_loss: 1.9913 - dense_21_loss: 1.9947 - dense_22_loss: 1.9861 - dense_23_loss: 1.9790 - dense_24_loss: 1.9890 - dense_25_loss: 1.9938 - dense_26_loss: 1.9932 - dense_27_loss: 1.9826 - dense_28_loss: 1.9747 - dense_29_loss: 1.9845 - dense_30_loss: 1.9509 - dense_31_loss: 1.9604 - dense_32_loss: 1.9185 - dense_33_loss: 1.8986 - dense_34_loss: 1.8757 - dense_35_loss: 1.8543 - dense_36_loss: 1.8047 - dense_37_loss: 1.7867 - dense_38_loss: 1.7596 - dense_39_loss: 1.7352 - dense_40_loss: 1.6983 - dense_41_loss: 1.6644 - dense_42_loss: 1.6217 - dense_43_loss: 1.5699 - dense_44_loss: 1.5062 - dense_45_loss: 1.4658 - dense_46_loss: 1.4216 - dense_47_loss: 1.3599 - dense_48_loss: 1.2987 - dense_49_loss: 1.2414 - dense_50_loss: 1.1767 - dense_51_loss: 1.0870 - dense_52_loss: 0.9888 - dense_53_loss: 0.8554 - dense_54_loss: 0.6600 - dense_55_loss: 0.4677 - dense_56_loss: 0.2699 - dense_57_loss: 0.1294 - dense_accuracy: 0.3932 - dense_1_accuracy: 0.3894 - dense_2_accuracy: 0.4404 - dense_3_accuracy: 0.4093 - dense_4_accuracy: 0.4123 - dense_5_accuracy: 0.4021 - dense_6_accuracy: 0.3854 - dense_7_accuracy: 0.3744 - dense_8_accuracy: 0.3726 - dense_9_accuracy: 0.3791 - dense_10_accuracy: 0.3970 - dense_11_accuracy: 0.3949 - dense_12_accuracy: 0.4024 - dense_13_accuracy: 0.4257 - dense_14_accuracy: 0.4358 - dense_15_accuracy: 0.4416 - dense_16_accuracy: 0.4262 - dense_17_accuracy: 0.4192 - dense_18_accuracy: 0.4057 - dense_19_accuracy: 0.3993 - dense_20_accuracy: 0.3906 - dense_21_accuracy: 0.3912 - dense_22_accuracy: 0.3903 - dense_23_accuracy: 0.3887 - dense_24_accuracy: 0.3803 - dense_25_accuracy: 0.3787 - dense_26_accuracy: 0.3723 - dense_27_accuracy: 0.3746 - dense_28_accuracy: 0.3692 - dense_29_accuracy: 0.3706 - dense_30_accuracy: 0.3706 - dense_31_accuracy: 0.3651 - dense_32_accuracy: 0.3721 - dense_33_accuracy: 0.3714 - dense_34_accuracy: 0.3678 - dense_35_accuracy: 0.3644 - dense_36_accuracy: 0.3759 - dense_37_accuracy: 0.3726 - dense_38_accuracy: 0.3695 - dense_39_accuracy: 0.3652 - dense_40_accuracy: 0.3611 - dense_41_accuracy: 0.3546 - dense_42_accuracy: 0.3469 - dense_43_accuracy: 0.3336 - dense_44_accuracy: 0.3263 - dense_45_accuracy: 0.3106 - dense_46_accuracy: 0.2899 - dense_47_accuracy: 0.2793 - dense_48_accuracy: 0.2635 - dense_49_accuracy: 0.2404 - dense_50_accuracy: 0.2244 - dense_51_accuracy: 0.2056 - dense_52_accuracy: 0.1901 - dense_53_accuracy: 0.1789 - dense_54_accuracy: 0.1826 - dense_55_accuracy: 0.1752 - dense_56_accuracy: 0.1625 - dense_57_accuracy: 0.1079\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32/23231 [..............................] - ETA: 0s - loss: 96.9297 - dense_loss: 1.8797 - dense_1_loss: 1.7353 - dense_2_loss: 1.7645 - dense_3_loss: 2.3763 - dense_4_loss: 2.2107 - dense_5_loss: 1.4066 - dense_6_loss: 2.0193 - dense_7_loss: 2.2195 - dense_8_loss: 2.0282 - dense_9_loss: 2.0032 - dense_10_loss: 1.9485 - dense_11_loss: 2.3477 - dense_12_loss: 1.6311 - dense_13_loss: 1.8775 - dense_14_loss: 1.6052 - dense_15_loss: 1.6895 - dense_16_loss: 1.5309 - dense_17_loss: 1.5333 - dense_18_loss: 1.8938 - dense_19_loss: 1.9135 - dense_20_loss: 1.7014 - dense_21_loss: 2.3264 - dense_22_loss: 2.3788 - dense_23_loss: 1.8910 - dense_24_loss: 1.8286 - dense_25_loss: 1.9705 - dense_26_loss: 2.2781 - dense_27_loss: 1.9238 - dense_28_loss: 2.3442 - dense_29_loss: 2.0608 - dense_30_loss: 1.9940 - dense_31_loss: 2.2175 - dense_32_loss: 2.1309 - dense_33_loss: 1.7868 - dense_34_loss: 1.7594 - dense_35_loss: 1.6716 - dense_36_loss: 1.5199 - dense_37_loss: 2.2375 - dense_38_loss: 1.2782 - dense_39_loss: 1.5956 - dense_40_loss: 1.9310 - dense_41_loss: 1.6648 - dense_42_loss: 1.3717 - dense_43_loss: 1.9759 - dense_44_loss: 1.3332 - dense_45_loss: 1.3146 - dense_46_loss: 1.3987 - dense_47_loss: 1.4940 - dense_48_loss: 1.5060 - dense_49_loss: 1.0156 - dense_50_loss: 1.1523 - dense_51_loss: 0.9057 - dense_52_loss: 0.7568 - dense_53_loss: 0.8811 - dense_54_loss: 0.7190 - dense_55_loss: 0.5806 - dense_56_loss: 0.2963 - dense_57_loss: 0.1229 - dense_accuracy: 0.3750 - dense_1_accuracy: 0.5625 - dense_2_accuracy: 0.4375 - dense_3_accuracy: 0.3438 - dense_4_accuracy: 0.3438 - dense_5_accuracy: 0.5312 - dense_6_accuracy: 0.5000 - dense_7_accuracy: 0.3438 - dense_8_accuracy: 0.4688 - dense_9_accuracy: 0.2500 - dense_10_accuracy: 0.4375 - dense_11_accuracy: 0.3750 - dense_12_accuracy: 0.4375 - dense_13_accuracy: 0.5000 - dense_14_accuracy: 0.4062 - dense_15_accuracy: 0.4688 - dense_16_accuracy: 0.5625 - dense_17_accuracy: 0.5000 - dense_18_accuracy: 0.3750 - dense_19_accuracy: 0.4062 - dense_20_accuracy: 0.5312 - dense_21_accuracy: 0.3125 - dense_22_accuracy: 0.3750 - dense_23_accuracy: 0.5000 - dense_24_accuracy: 0.4375 - dense_25_accuracy: 0.4375 - dense_26_accuracy: 0.3438 - dense_27_accuracy: 0.3438 - dense_28_accuracy: 0.3125 - dense_29_accuracy: 0.3750 - dense_30_accuracy: 0.4062 - dense_31_accuracy: 0.4688 - dense_32_accuracy: 0.4062 - dense_33_accuracy: 0.4062 - dense_34_accuracy: 0.4062 - dense_35_accuracy: 0.4062 - dense_36_accuracy: 0.4375 - dense_37_accuracy: 0.3750 - dense_38_accuracy: 0.5938 - dense_39_accuracy: 0.4688 - dense_40_accuracy: 0.3125 - dense_41_accuracy: 0.3750 - dense_42_accuracy: 0.3438 - dense_43_accuracy: 0.1875 - dense_44_accuracy: 0.3438 - dense_45_accuracy: 0.3438 - dense_46_accuracy: 0.3750 - dense_47_accuracy: 0.1875 - dense_48_accuracy: 0.2188 - dense_49_accuracy: 0.1875 - dense_50_accuracy: 0.2500 - dense_51_accuracy: 0.2188 - dense_52_accuracy: 0.1875 - dense_53_accuracy: 0.1562 - dense_54_accuracy: 0.2188 - dense_55_accuracy: 0.1875 - dense_56_accuracy: 0.2500 - dense_57_accuracy: 0.1250\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "   96/23231 [..............................] - ETA: 18s - loss: 98.1832 - dense_loss: 2.0113 - dense_1_loss: 1.9121 - dense_2_loss: 1.9152 - dense_3_loss: 2.2760 - dense_4_loss: 2.0361 - dense_5_loss: 1.7618 - dense_6_loss: 2.0255 - dense_7_loss: 2.1557 - dense_8_loss: 1.9542 - dense_9_loss: 2.0603 - dense_10_loss: 1.9327 - dense_11_loss: 2.0921 - dense_12_loss: 1.8472 - dense_13_loss: 1.8878 - dense_14_loss: 1.8225 - dense_15_loss: 1.7746 - dense_16_loss: 1.8404 - dense_17_loss: 1.8825 - dense_18_loss: 1.8740 - dense_19_loss: 1.9205 - dense_20_loss: 2.0111 - dense_21_loss: 1.9902 - dense_22_loss: 2.2563 - dense_23_loss: 2.0323 - dense_24_loss: 1.7767 - dense_25_loss: 1.9804 - dense_26_loss: 2.2014 - dense_27_loss: 1.7038 - dense_28_loss: 2.2951 - dense_29_loss: 1.9738 - dense_30_loss: 1.8827 - dense_31_loss: 1.7537 - dense_32_loss: 1.9706 - dense_33_loss: 1.7982 - dense_34_loss: 1.6580 - dense_35_loss: 1.6035 - dense_36_loss: 1.8256 - dense_37_loss: 1.8312 - dense_38_loss: 1.4590 - dense_39_loss: 1.7965 - dense_40_loss: 1.9746 - dense_41_loss: 1.7239 - dense_42_loss: 1.6019 - dense_43_loss: 1.7064 - dense_44_loss: 1.5726 - dense_45_loss: 1.4801 - dense_46_loss: 1.3763 - dense_47_loss: 1.5905 - dense_48_loss: 1.4707 - dense_49_loss: 1.2574 - dense_50_loss: 1.1474 - dense_51_loss: 0.9104 - dense_52_loss: 0.8358 - dense_53_loss: 0.9907 - dense_54_loss: 0.8138 - dense_55_loss: 0.5058 - dense_56_loss: 0.2767 - dense_57_loss: 0.1657 - dense_accuracy: 0.3021 - dense_1_accuracy: 0.4479 - dense_2_accuracy: 0.4583 - dense_3_accuracy: 0.3542 - dense_4_accuracy: 0.3438 - dense_5_accuracy: 0.4583 - dense_6_accuracy: 0.4375 - dense_7_accuracy: 0.2917 - dense_8_accuracy: 0.3958 - dense_9_accuracy: 0.3542 - dense_10_accuracy: 0.3854 - dense_11_accuracy: 0.3854 - dense_12_accuracy: 0.3958 - dense_13_accuracy: 0.4583 - dense_14_accuracy: 0.4167 - dense_15_accuracy: 0.4375 - dense_16_accuracy: 0.4792 - dense_17_accuracy: 0.3958 - dense_18_accuracy: 0.3854 - dense_19_accuracy: 0.4375 - dense_20_accuracy: 0.3438 - dense_21_accuracy: 0.3438 - dense_22_accuracy: 0.3229 - dense_23_accuracy: 0.3750 - dense_24_accuracy: 0.4688 - dense_25_accuracy: 0.3958 - dense_26_accuracy: 0.3333 - dense_27_accuracy: 0.4479 - dense_28_accuracy: 0.3125 - dense_29_accuracy: 0.3229 - dense_30_accuracy: 0.3958 - dense_31_accuracy: 0.4792 - dense_32_accuracy: 0.3854 - dense_33_accuracy: 0.3750 - dense_34_accuracy: 0.4583 - dense_35_accuracy: 0.4688 - dense_36_accuracy: 0.3333 - dense_37_accuracy: 0.3958 - dense_38_accuracy: 0.5312 - dense_39_accuracy: 0.3958 - dense_40_accuracy: 0.3333 - dense_41_accuracy: 0.3646 - dense_42_accuracy: 0.3229 - dense_43_accuracy: 0.3021 - dense_44_accuracy: 0.3750 - dense_45_accuracy: 0.3229 - dense_46_accuracy: 0.3750 - dense_47_accuracy: 0.2292 - dense_48_accuracy: 0.2500 - dense_49_accuracy: 0.2396 - dense_50_accuracy: 0.2500 - dense_51_accuracy: 0.2292 - dense_52_accuracy: 0.2083 - dense_53_accuracy: 0.1562 - dense_54_accuracy: 0.1562 - dense_55_accuracy: 0.1562 - dense_56_accuracy: 0.2188 - dense_57_accuracy: 0.0938"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23231/23231 [==============================] - 27s 1ms/sample - loss: 97.5366 - dense_loss: 1.9224 - dense_1_loss: 2.0657 - dense_2_loss: 1.9401 - dense_3_loss: 2.0840 - dense_4_loss: 2.0229 - dense_5_loss: 2.0347 - dense_6_loss: 2.0771 - dense_7_loss: 2.0705 - dense_8_loss: 2.0609 - dense_9_loss: 2.0277 - dense_10_loss: 1.9611 - dense_11_loss: 1.9523 - dense_12_loss: 1.9282 - dense_13_loss: 1.8620 - dense_14_loss: 1.8483 - dense_15_loss: 1.8532 - dense_16_loss: 1.9014 - dense_17_loss: 1.9209 - dense_18_loss: 1.9431 - dense_19_loss: 1.9513 - dense_20_loss: 1.9660 - dense_21_loss: 1.9684 - dense_22_loss: 1.9614 - dense_23_loss: 1.9529 - dense_24_loss: 1.9649 - dense_25_loss: 1.9685 - dense_26_loss: 1.9666 - dense_27_loss: 1.9576 - dense_28_loss: 1.9498 - dense_29_loss: 1.9611 - dense_30_loss: 1.9229 - dense_31_loss: 1.9334 - dense_32_loss: 1.8946 - dense_33_loss: 1.8708 - dense_34_loss: 1.8484 - dense_35_loss: 1.8290 - dense_36_loss: 1.7808 - dense_37_loss: 1.7622 - dense_38_loss: 1.7346 - dense_39_loss: 1.7117 - dense_40_loss: 1.6762 - dense_41_loss: 1.6436 - dense_42_loss: 1.6010 - dense_43_loss: 1.5496 - dense_44_loss: 1.4893 - dense_45_loss: 1.4456 - dense_46_loss: 1.4017 - dense_47_loss: 1.3425 - dense_48_loss: 1.2809 - dense_49_loss: 1.2239 - dense_50_loss: 1.1605 - dense_51_loss: 1.0719 - dense_52_loss: 0.9736 - dense_53_loss: 0.8409 - dense_54_loss: 0.6481 - dense_55_loss: 0.4585 - dense_56_loss: 0.2681 - dense_57_loss: 0.1274 - dense_accuracy: 0.3941 - dense_1_accuracy: 0.3975 - dense_2_accuracy: 0.4533 - dense_3_accuracy: 0.4191 - dense_4_accuracy: 0.4237 - dense_5_accuracy: 0.4106 - dense_6_accuracy: 0.3955 - dense_7_accuracy: 0.3828 - dense_8_accuracy: 0.3809 - dense_9_accuracy: 0.3894 - dense_10_accuracy: 0.4056 - dense_11_accuracy: 0.4048 - dense_12_accuracy: 0.4129 - dense_13_accuracy: 0.4387 - dense_14_accuracy: 0.4479 - dense_15_accuracy: 0.4479 - dense_16_accuracy: 0.4361 - dense_17_accuracy: 0.4284 - dense_18_accuracy: 0.4149 - dense_19_accuracy: 0.4099 - dense_20_accuracy: 0.3984 - dense_21_accuracy: 0.3952 - dense_22_accuracy: 0.3975 - dense_23_accuracy: 0.3956 - dense_24_accuracy: 0.3856 - dense_25_accuracy: 0.3837 - dense_26_accuracy: 0.3811 - dense_27_accuracy: 0.3818 - dense_28_accuracy: 0.3785 - dense_29_accuracy: 0.3770 - dense_30_accuracy: 0.3802 - dense_31_accuracy: 0.3757 - dense_32_accuracy: 0.3808 - dense_33_accuracy: 0.3797 - dense_34_accuracy: 0.3736 - dense_35_accuracy: 0.3698 - dense_36_accuracy: 0.3833 - dense_37_accuracy: 0.3789 - dense_38_accuracy: 0.3754 - dense_39_accuracy: 0.3711 - dense_40_accuracy: 0.3657 - dense_41_accuracy: 0.3584 - dense_42_accuracy: 0.3511 - dense_43_accuracy: 0.3420 - dense_44_accuracy: 0.3321 - dense_45_accuracy: 0.3156 - dense_46_accuracy: 0.2989 - dense_47_accuracy: 0.2857 - dense_48_accuracy: 0.2684 - dense_49_accuracy: 0.2465 - dense_50_accuracy: 0.2288 - dense_51_accuracy: 0.2100 - dense_52_accuracy: 0.1954 - dense_53_accuracy: 0.1828 - dense_54_accuracy: 0.1874 - dense_55_accuracy: 0.1779 - dense_56_accuracy: 0.1607 - dense_57_accuracy: 0.1083\n",
      "Epoch 10/100\n",
      "12256/23231 [==============>...............] - ETA: 12s - loss: 96.4016 - dense_loss: 1.9156 - dense_1_loss: 2.0392 - dense_2_loss: 1.9011 - dense_3_loss: 2.0448 - dense_4_loss: 1.9997 - dense_5_loss: 2.0182 - dense_6_loss: 2.0637 - dense_7_loss: 2.0517 - dense_8_loss: 2.0353 - dense_9_loss: 1.9971 - dense_10_loss: 1.9380 - dense_11_loss: 1.9422 - dense_12_loss: 1.9146 - dense_13_loss: 1.8461 - dense_14_loss: 1.8113 - dense_15_loss: 1.8315 - dense_16_loss: 1.8733 - dense_17_loss: 1.9046 - dense_18_loss: 1.9216 - dense_19_loss: 1.9418 - dense_20_loss: 1.9477 - dense_21_loss: 1.9499 - dense_22_loss: 1.9369 - dense_23_loss: 1.9108 - dense_24_loss: 1.9429 - dense_25_loss: 1.9371 - dense_26_loss: 1.9587 - dense_27_loss: 1.9352 - dense_28_loss: 1.9253 - dense_29_loss: 1.9292 - dense_30_loss: 1.9005 - dense_31_loss: 1.9183 - dense_32_loss: 1.8719 - dense_33_loss: 1.8502 - dense_34_loss: 1.8290 - dense_35_loss: 1.8008 - dense_36_loss: 1.7515 - dense_37_loss: 1.7318 - dense_38_loss: 1.7187 - dense_39_loss: 1.6878 - dense_40_loss: 1.6688 - dense_41_loss: 1.6361 - dense_42_loss: 1.5925 - dense_43_loss: 1.5284 - dense_44_loss: 1.4574 - dense_45_loss: 1.4264 - dense_46_loss: 1.3789 - dense_47_loss: 1.3267 - dense_48_loss: 1.2645 - dense_49_loss: 1.2084 - dense_50_loss: 1.1401 - dense_51_loss: 1.0573 - dense_52_loss: 0.9645 - dense_53_loss: 0.8228 - dense_54_loss: 0.6431 - dense_55_loss: 0.4587 - dense_56_loss: 0.2715 - dense_57_loss: 0.1298 - dense_accuracy: 0.3928 - dense_1_accuracy: 0.4021 - dense_2_accuracy: 0.4652 - dense_3_accuracy: 0.4318 - dense_4_accuracy: 0.4273 - dense_5_accuracy: 0.4174 - dense_6_accuracy: 0.4031 - dense_7_accuracy: 0.3931 - dense_8_accuracy: 0.3854 - dense_9_accuracy: 0.4011 - dense_10_accuracy: 0.4117 - dense_11_accuracy: 0.4049 - dense_12_accuracy: 0.4166 - dense_13_accuracy: 0.4479 - dense_14_accuracy: 0.4549 - dense_15_accuracy: 0.4562 - dense_16_accuracy: 0.4426 - dense_17_accuracy: 0.4300 - dense_18_accuracy: 0.4148 - dense_19_accuracy: 0.4149 - dense_20_accuracy: 0.4053 - dense_21_accuracy: 0.3999 - dense_22_accuracy: 0.4048 - dense_23_accuracy: 0.4071 - dense_24_accuracy: 0.3889 - dense_25_accuracy: 0.3948 - dense_26_accuracy: 0.3833 - dense_27_accuracy: 0.3943 - dense_28_accuracy: 0.3903 - dense_29_accuracy: 0.3857 - dense_30_accuracy: 0.3833 - dense_31_accuracy: 0.3742 - dense_32_accuracy: 0.3858 - dense_33_accuracy: 0.3880 - dense_34_accuracy: 0.3834 - dense_35_accuracy: 0.3782 - dense_36_accuracy: 0.3935 - dense_37_accuracy: 0.3874 - dense_38_accuracy: 0.3784 - dense_39_accuracy: 0.3739 - dense_40_accuracy: 0.3661 - dense_41_accuracy: 0.3603 - dense_42_accuracy: 0.3530 - dense_43_accuracy: 0.3446 - dense_44_accuracy: 0.3433 - dense_45_accuracy: 0.3196 - dense_46_accuracy: 0.2990 - dense_47_accuracy: 0.2865 - dense_48_accuracy: 0.2687 - dense_49_accuracy: 0.2482 - dense_50_accuracy: 0.2312 - dense_51_accuracy: 0.2130 - dense_52_accuracy: 0.1932 - dense_53_accuracy: 0.1864 - dense_54_accuracy: 0.1899 - dense_55_accuracy: 0.1808 - dense_56_accuracy: 0.1625 - dense_57_accuracy: 0.111"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b85780813c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;31m# Callbacks batch end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    287\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    307\u001b[0m       \u001b[0mbatch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    340\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' - %s:'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m           \u001b[0mavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mavg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([X, a0, c0], list(Y), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x):\n",
    "    x = K.argmax(x)\n",
    "    x = tf.one_hot(indices=x, depth=78) \n",
    "    x = RepeatVector(1)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_model(LSTM_cell, prediction_layer, vocab_size = vocab_size, n_a = n_a, length = 58):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of values.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    n_values -- integer, number of unique values\n",
    "    n_a -- number of units in the LSTM_cell\n",
    "    Ty -- integer, number of time steps to generate\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape \n",
    "    x0 = Input(shape=(1, vocab_size))\n",
    "    \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "    \n",
    "    #densor and lstm cell are already trained\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create an empty list of \"outputs\" to later store your predicted values (≈1 line)\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 2: Loop over Ty and generate a value at every time step\n",
    "    for t in range(length):\n",
    "        \n",
    "        # Step 2.A: Perform one step of LSTM_cell (≈1 line)\n",
    "        a, _, c = LSTM_cell(x, initial_state=[a, c])\n",
    "        \n",
    "        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)\n",
    "        out = prediction_layer(a) #output y?\n",
    "        #make sure sampling is applied according to distribution?\n",
    "\n",
    "        # Step 2.C: Append the prediction \"out\" to \"outputs\". out.shape = (None, 78) (≈1 line)\n",
    "        outputs.append(out)\n",
    "        \n",
    "        # Step 2.D: \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        # See instructions above.\n",
    "        x = Lambda(one_hot)(out) #update x to be one hot rep of last output y\n",
    "        \n",
    "    # Step 3: Create model instance with the correct \"inputs\" and \"outputs\" (≈1 line)\n",
    "    inference_model = Model(inputs= [x0,a0,c0], outputs=outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return inference_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model = inference_model(LSTM_cell, prediction_layer, vocab_size = vocab_size, n_a = n_a, length = 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
